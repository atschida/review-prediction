{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>product_parent</th>\n",
       "      <th>positive</th>\n",
       "      <th>cust_review_count</th>\n",
       "      <th>cust_star_rating_mean</th>\n",
       "      <th>cust_star_rating_std</th>\n",
       "      <th>prod_review_count</th>\n",
       "      <th>prod_star_rating_mean</th>\n",
       "      <th>prod_star_rating_std</th>\n",
       "      <th>pos_sim</th>\n",
       "      <th>...</th>\n",
       "      <th>pos_3_word_3</th>\n",
       "      <th>neg_1_word_1</th>\n",
       "      <th>neg_1_word_2</th>\n",
       "      <th>neg_1_word_3</th>\n",
       "      <th>neg_2_word_1</th>\n",
       "      <th>neg_2_word_2</th>\n",
       "      <th>neg_2_word_3</th>\n",
       "      <th>neg_3_word_1</th>\n",
       "      <th>neg_3_word_2</th>\n",
       "      <th>neg_3_word_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1617361</td>\n",
       "      <td>849246716</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>297</td>\n",
       "      <td>4.872054</td>\n",
       "      <td>0.363625</td>\n",
       "      <td>0.033451</td>\n",
       "      <td>...</td>\n",
       "      <td>0.186957</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53052607</td>\n",
       "      <td>849246716</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>0.547723</td>\n",
       "      <td>297</td>\n",
       "      <td>4.872054</td>\n",
       "      <td>0.363625</td>\n",
       "      <td>0.012478</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028547</td>\n",
       "      <td>0.172017</td>\n",
       "      <td>0.084518</td>\n",
       "      <td>0.194290</td>\n",
       "      <td>0.002472</td>\n",
       "      <td>0.006273</td>\n",
       "      <td>0.052950</td>\n",
       "      <td>0.344210</td>\n",
       "      <td>0.307019</td>\n",
       "      <td>0.363920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15679577</td>\n",
       "      <td>849246716</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>3.375000</td>\n",
       "      <td>1.505941</td>\n",
       "      <td>297</td>\n",
       "      <td>4.872054</td>\n",
       "      <td>0.363625</td>\n",
       "      <td>0.007220</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044000</td>\n",
       "      <td>0.371702</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.316641</td>\n",
       "      <td>0.156807</td>\n",
       "      <td>0.052990</td>\n",
       "      <td>0.165214</td>\n",
       "      <td>0.011699</td>\n",
       "      <td>-0.066285</td>\n",
       "      <td>-0.171195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16367779</td>\n",
       "      <td>849246716</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>4.444444</td>\n",
       "      <td>0.527046</td>\n",
       "      <td>297</td>\n",
       "      <td>4.872054</td>\n",
       "      <td>0.363625</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.171195</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25485198</td>\n",
       "      <td>849246716</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>297</td>\n",
       "      <td>4.872054</td>\n",
       "      <td>0.363625</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>...</td>\n",
       "      <td>0.249734</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id  product_parent  positive  cust_review_count  \\\n",
       "0      1617361       849246716         1                  5   \n",
       "1     53052607       849246716         0                  5   \n",
       "2     15679577       849246716         1                  8   \n",
       "3     16367779       849246716         1                  9   \n",
       "4     25485198       849246716         1                 17   \n",
       "\n",
       "   cust_star_rating_mean  cust_star_rating_std  prod_review_count  \\\n",
       "0               5.000000              0.000000                297   \n",
       "1               3.600000              0.547723                297   \n",
       "2               3.375000              1.505941                297   \n",
       "3               4.444444              0.527046                297   \n",
       "4               5.000000              0.000000                297   \n",
       "\n",
       "   prod_star_rating_mean  prod_star_rating_std   pos_sim  ...  pos_3_word_3  \\\n",
       "0               4.872054              0.363625  0.033451  ...      0.186957   \n",
       "1               4.872054              0.363625  0.012478  ...     -0.028547   \n",
       "2               4.872054              0.363625  0.007220  ...      0.044000   \n",
       "3               4.872054              0.363625  0.095238  ...     -0.171195   \n",
       "4               4.872054              0.363625  0.010870  ...      0.249734   \n",
       "\n",
       "   neg_1_word_1  neg_1_word_2  neg_1_word_3  neg_2_word_1  neg_2_word_2  \\\n",
       "0      0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "1      0.172017      0.084518      0.194290      0.002472      0.006273   \n",
       "2      0.371702      0.000015      0.316641      0.156807      0.052990   \n",
       "3      0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "4      0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "\n",
       "   neg_2_word_3  neg_3_word_1  neg_3_word_2  neg_3_word_3  \n",
       "0      0.000000      0.000000      0.000000      0.000000  \n",
       "1      0.052950      0.344210      0.307019      0.363920  \n",
       "2      0.165214      0.011699     -0.066285     -0.171195  \n",
       "3      0.000000      0.000000      0.000000      0.000000  \n",
       "4      0.000000      0.000000      0.000000      0.000000  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('train_df2.csv', index_col=0)\n",
    "train_df.drop(['star_rating', 'pos_corpus', 'neg_corpus', 'pos_1', 'pos_2', 'pos_3', \n",
    "               'neg_1', 'neg_2', 'neg_3', 'prod_corpus', 'word_1', 'word_2', 'word_3'], axis=1, inplace=True)\n",
    "train_df.drop(['cust_total_votes_mean', 'cust_total_votes_std', 'cust_helpful_votes_mean', 'cust_helpful_votes_std', \n",
    "               'prod_total_votes_mean', 'prod_total_votes_std', 'prod_helpful_votes_mean', 'prod_helpful_votes_std'], \n",
    "              axis=1, inplace=True)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['customer_id', 'product_parent', 'positive', 'cust_review_count',\n",
       "       'cust_star_rating_mean', 'cust_star_rating_std', 'prod_review_count',\n",
       "       'prod_star_rating_mean', 'prod_star_rating_std', 'pos_sim', 'neg_sim',\n",
       "       'pos_1_word_1', 'pos_1_word_2', 'pos_1_word_3', 'pos_2_word_1',\n",
       "       'pos_2_word_2', 'pos_2_word_3', 'pos_3_word_1', 'pos_3_word_2',\n",
       "       'pos_3_word_3', 'neg_1_word_1', 'neg_1_word_2', 'neg_1_word_3',\n",
       "       'neg_2_word_1', 'neg_2_word_2', 'neg_2_word_3', 'neg_3_word_1',\n",
       "       'neg_3_word_2', 'neg_3_word_3'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive\n",
       "0     9531\n",
       "1    38191\n",
       "dtype: int64"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.groupby('positive').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = train_df[train_df.positive == 0]\n",
    "temp = pd.concat([temp, train_df[train_df.positive == 1][:9531]])\n",
    "train_df = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive\n",
       "0    0.5\n",
       "1    0.5\n",
       "dtype: float64"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.groupby('positive').size() / len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>product_parent</th>\n",
       "      <th>positive</th>\n",
       "      <th>cust_review_count</th>\n",
       "      <th>cust_star_rating_mean</th>\n",
       "      <th>cust_star_rating_std</th>\n",
       "      <th>prod_review_count</th>\n",
       "      <th>prod_star_rating_mean</th>\n",
       "      <th>prod_star_rating_std</th>\n",
       "      <th>pos_sim</th>\n",
       "      <th>...</th>\n",
       "      <th>pos_3_word_3</th>\n",
       "      <th>neg_1_word_1</th>\n",
       "      <th>neg_1_word_2</th>\n",
       "      <th>neg_1_word_3</th>\n",
       "      <th>neg_2_word_1</th>\n",
       "      <th>neg_2_word_2</th>\n",
       "      <th>neg_2_word_3</th>\n",
       "      <th>neg_3_word_1</th>\n",
       "      <th>neg_3_word_2</th>\n",
       "      <th>neg_3_word_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36330222</td>\n",
       "      <td>986428010</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>4.291667</td>\n",
       "      <td>1.267629</td>\n",
       "      <td>18</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>1.188177</td>\n",
       "      <td>0.070423</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001941</td>\n",
       "      <td>0.000987</td>\n",
       "      <td>-0.015594</td>\n",
       "      <td>-0.013630</td>\n",
       "      <td>0.006534</td>\n",
       "      <td>0.007653</td>\n",
       "      <td>0.023873</td>\n",
       "      <td>-0.029103</td>\n",
       "      <td>-0.046843</td>\n",
       "      <td>-0.041810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24360083</td>\n",
       "      <td>986428010</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>4.347826</td>\n",
       "      <td>1.191206</td>\n",
       "      <td>18</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>1.188177</td>\n",
       "      <td>0.095361</td>\n",
       "      <td>...</td>\n",
       "      <td>0.901494</td>\n",
       "      <td>-0.222196</td>\n",
       "      <td>-0.196039</td>\n",
       "      <td>-0.217517</td>\n",
       "      <td>0.918800</td>\n",
       "      <td>0.925143</td>\n",
       "      <td>0.901494</td>\n",
       "      <td>0.919331</td>\n",
       "      <td>0.902017</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28891040</td>\n",
       "      <td>437083384</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>407</td>\n",
       "      <td>4.503686</td>\n",
       "      <td>0.887439</td>\n",
       "      <td>0.006944</td>\n",
       "      <td>...</td>\n",
       "      <td>0.092532</td>\n",
       "      <td>0.313854</td>\n",
       "      <td>0.086877</td>\n",
       "      <td>0.315884</td>\n",
       "      <td>0.196752</td>\n",
       "      <td>0.052950</td>\n",
       "      <td>0.194290</td>\n",
       "      <td>0.054040</td>\n",
       "      <td>0.014332</td>\n",
       "      <td>-0.002904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>52449052</td>\n",
       "      <td>437083384</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>1.516575</td>\n",
       "      <td>407</td>\n",
       "      <td>4.503686</td>\n",
       "      <td>0.887439</td>\n",
       "      <td>0.010390</td>\n",
       "      <td>...</td>\n",
       "      <td>0.187860</td>\n",
       "      <td>0.083284</td>\n",
       "      <td>-0.115922</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.090283</td>\n",
       "      <td>-0.083403</td>\n",
       "      <td>0.025464</td>\n",
       "      <td>0.084633</td>\n",
       "      <td>-0.092710</td>\n",
       "      <td>0.019767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27192976</td>\n",
       "      <td>437083384</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>0.816497</td>\n",
       "      <td>407</td>\n",
       "      <td>4.503686</td>\n",
       "      <td>0.887439</td>\n",
       "      <td>0.012942</td>\n",
       "      <td>...</td>\n",
       "      <td>0.126235</td>\n",
       "      <td>-0.083963</td>\n",
       "      <td>0.036128</td>\n",
       "      <td>-0.155615</td>\n",
       "      <td>-0.020984</td>\n",
       "      <td>0.021015</td>\n",
       "      <td>-0.042505</td>\n",
       "      <td>0.059809</td>\n",
       "      <td>-0.017769</td>\n",
       "      <td>-0.064662</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id  product_parent  positive  cust_review_count  \\\n",
       "0     36330222       986428010         1                 24   \n",
       "1     24360083       986428010         1                 23   \n",
       "2     28891040       437083384         1                 10   \n",
       "3     52449052       437083384         0                  5   \n",
       "4     27192976       437083384         1                  6   \n",
       "\n",
       "   cust_star_rating_mean  cust_star_rating_std  prod_review_count  \\\n",
       "0               4.291667              1.267629                 18   \n",
       "1               4.347826              1.191206                 18   \n",
       "2               4.500000              0.707107                407   \n",
       "3               3.400000              1.516575                407   \n",
       "4               4.666667              0.816497                407   \n",
       "\n",
       "   prod_star_rating_mean  prod_star_rating_std   pos_sim  ...  pos_3_word_3  \\\n",
       "0               4.333333              1.188177  0.070423  ...      0.001941   \n",
       "1               4.333333              1.188177  0.095361  ...      0.901494   \n",
       "2               4.503686              0.887439  0.006944  ...      0.092532   \n",
       "3               4.503686              0.887439  0.010390  ...      0.187860   \n",
       "4               4.503686              0.887439  0.012942  ...      0.126235   \n",
       "\n",
       "   neg_1_word_1  neg_1_word_2  neg_1_word_3  neg_2_word_1  neg_2_word_2  \\\n",
       "0      0.000987     -0.015594     -0.013630      0.006534      0.007653   \n",
       "1     -0.222196     -0.196039     -0.217517      0.918800      0.925143   \n",
       "2      0.313854      0.086877      0.315884      0.196752      0.052950   \n",
       "3      0.083284     -0.115922      0.023810      0.090283     -0.083403   \n",
       "4     -0.083963      0.036128     -0.155615     -0.020984      0.021015   \n",
       "\n",
       "   neg_2_word_3  neg_3_word_1  neg_3_word_2  neg_3_word_3  \n",
       "0      0.023873     -0.029103     -0.046843     -0.041810  \n",
       "1      0.901494      0.919331      0.902017      1.000000  \n",
       "2      0.194290      0.054040      0.014332     -0.002904  \n",
       "3      0.025464      0.084633     -0.092710      0.019767  \n",
       "4     -0.042505      0.059809     -0.017769     -0.064662  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('test_df2.csv', index_col=0)\n",
    "test_df.drop(['star_rating', 'pos_corpus', 'neg_corpus', 'pos_1', 'pos_2', 'pos_3', \n",
    "               'neg_1', 'neg_2', 'neg_3', 'prod_corpus', 'word_1', 'word_2', 'word_3'], axis=1, inplace=True)\n",
    "test_df.drop(['cust_total_votes_mean', 'cust_total_votes_std', 'cust_helpful_votes_mean', 'cust_helpful_votes_std', \n",
    "               'prod_total_votes_mean', 'prod_total_votes_std', 'prod_helpful_votes_mean', 'prod_helpful_votes_std'], \n",
    "              axis=1, inplace=True)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive\n",
       "0     3573\n",
       "1    12262\n",
       "dtype: int64"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.groupby('positive').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = test_df[test_df.positive == 0]\n",
    "temp = pd.concat([temp, test_df[test_df.positive == 1][:3573]])\n",
    "test_df = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive\n",
       "0    0.5\n",
       "1    0.5\n",
       "dtype: float64"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.groupby('positive').size() / len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_df.query('positive == 1')) / len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = train_df[['customer_id', 'product_parent', 'positive', 'pos_sim', 'neg_sim',\n",
    "        'pos_2_word_1', 'pos_2_word_2', 'pos_2_word_3', 'pos_3_word_1',\n",
    "        'pos_3_word_2', 'pos_3_word_3', 'neg_1_word_1', 'neg_1_word_2',\n",
    "        'neg_1_word_3', 'neg_2_word_1', 'neg_2_word_2', 'neg_2_word_3',\n",
    "        'neg_3_word_1', 'neg_3_word_2', 'neg_3_word_3']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>product_parent</th>\n",
       "      <th>positive</th>\n",
       "      <th>cust_review_count</th>\n",
       "      <th>cust_star_rating_mean</th>\n",
       "      <th>cust_star_rating_std</th>\n",
       "      <th>prod_review_count</th>\n",
       "      <th>prod_star_rating_mean</th>\n",
       "      <th>prod_star_rating_std</th>\n",
       "      <th>pos_sim</th>\n",
       "      <th>...</th>\n",
       "      <th>pos_3_word_3</th>\n",
       "      <th>neg_1_word_1</th>\n",
       "      <th>neg_1_word_2</th>\n",
       "      <th>neg_1_word_3</th>\n",
       "      <th>neg_2_word_1</th>\n",
       "      <th>neg_2_word_2</th>\n",
       "      <th>neg_2_word_3</th>\n",
       "      <th>neg_3_word_1</th>\n",
       "      <th>neg_3_word_2</th>\n",
       "      <th>neg_3_word_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53052607</td>\n",
       "      <td>849246716</td>\n",
       "      <td>0</td>\n",
       "      <td>0.009662</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.237171</td>\n",
       "      <td>0.047284</td>\n",
       "      <td>0.968013</td>\n",
       "      <td>0.128561</td>\n",
       "      <td>0.012478</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028547</td>\n",
       "      <td>0.172017</td>\n",
       "      <td>0.084518</td>\n",
       "      <td>0.194290</td>\n",
       "      <td>0.002472</td>\n",
       "      <td>0.006273</td>\n",
       "      <td>0.052950</td>\n",
       "      <td>0.344210</td>\n",
       "      <td>0.307019</td>\n",
       "      <td>0.363920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>53002620</td>\n",
       "      <td>927530974</td>\n",
       "      <td>0</td>\n",
       "      <td>0.101449</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.729800</td>\n",
       "      <td>0.049681</td>\n",
       "      <td>0.837340</td>\n",
       "      <td>0.384026</td>\n",
       "      <td>0.059715</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003949</td>\n",
       "      <td>0.000987</td>\n",
       "      <td>-0.013630</td>\n",
       "      <td>-0.015594</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.919331</td>\n",
       "      <td>0.916021</td>\n",
       "      <td>-0.080104</td>\n",
       "      <td>-0.048943</td>\n",
       "      <td>-0.067588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>20846010</td>\n",
       "      <td>927530974</td>\n",
       "      <td>0</td>\n",
       "      <td>0.028986</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.718070</td>\n",
       "      <td>0.049681</td>\n",
       "      <td>0.837340</td>\n",
       "      <td>0.384026</td>\n",
       "      <td>0.008672</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008186</td>\n",
       "      <td>0.100507</td>\n",
       "      <td>0.110014</td>\n",
       "      <td>0.119207</td>\n",
       "      <td>-0.067333</td>\n",
       "      <td>-0.113143</td>\n",
       "      <td>-0.060759</td>\n",
       "      <td>0.110559</td>\n",
       "      <td>0.102211</td>\n",
       "      <td>0.097801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>19037225</td>\n",
       "      <td>927530974</td>\n",
       "      <td>0</td>\n",
       "      <td>0.019324</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.163663</td>\n",
       "      <td>0.049681</td>\n",
       "      <td>0.837340</td>\n",
       "      <td>0.384026</td>\n",
       "      <td>0.014129</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.078425</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>8129762</td>\n",
       "      <td>927530974</td>\n",
       "      <td>0</td>\n",
       "      <td>0.009662</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>0.049681</td>\n",
       "      <td>0.837340</td>\n",
       "      <td>0.384026</td>\n",
       "      <td>0.002285</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.278179</td>\n",
       "      <td>-0.135831</td>\n",
       "      <td>-0.137193</td>\n",
       "      <td>-0.123986</td>\n",
       "      <td>-0.039616</td>\n",
       "      <td>-0.044543</td>\n",
       "      <td>-0.051168</td>\n",
       "      <td>0.142871</td>\n",
       "      <td>0.144287</td>\n",
       "      <td>0.145192</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    customer_id  product_parent  positive  cust_review_count  \\\n",
       "1      53052607       849246716         0           0.009662   \n",
       "28     53002620       927530974         0           0.101449   \n",
       "31     20846010       927530974         0           0.028986   \n",
       "36     19037225       927530974         0           0.019324   \n",
       "37      8129762       927530974         0           0.009662   \n",
       "\n",
       "    cust_star_rating_mean  cust_star_rating_std  prod_review_count  \\\n",
       "1                0.650000              0.237171           0.047284   \n",
       "28               0.416667              0.729800           0.049681   \n",
       "31               0.583333              0.718070           0.049681   \n",
       "36               0.785714              0.163663           0.049681   \n",
       "37               0.750000              0.530330           0.049681   \n",
       "\n",
       "    prod_star_rating_mean  prod_star_rating_std   pos_sim  ...  pos_3_word_3  \\\n",
       "1                0.968013              0.128561  0.012478  ...     -0.028547   \n",
       "28               0.837340              0.384026  0.059715  ...     -0.003949   \n",
       "31               0.837340              0.384026  0.008672  ...      0.008186   \n",
       "36               0.837340              0.384026  0.014129  ...     -0.078425   \n",
       "37               0.837340              0.384026  0.002285  ...     -0.278179   \n",
       "\n",
       "    neg_1_word_1  neg_1_word_2  neg_1_word_3  neg_2_word_1  neg_2_word_2  \\\n",
       "1       0.172017      0.084518      0.194290      0.002472      0.006273   \n",
       "28      0.000987     -0.013630     -0.015594      1.000000      0.919331   \n",
       "31      0.100507      0.110014      0.119207     -0.067333     -0.113143   \n",
       "36      0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "37     -0.135831     -0.137193     -0.123986     -0.039616     -0.044543   \n",
       "\n",
       "    neg_2_word_3  neg_3_word_1  neg_3_word_2  neg_3_word_3  \n",
       "1       0.052950      0.344210      0.307019      0.363920  \n",
       "28      0.916021     -0.080104     -0.048943     -0.067588  \n",
       "31     -0.060759      0.110559      0.102211      0.097801  \n",
       "36      0.000000      0.000000      0.000000      0.000000  \n",
       "37     -0.051168      0.142871      0.144287      0.145192  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "train_df_norm = pd.DataFrame(min_max_scaler.fit_transform(train_df), columns=train_df.columns, index=train_df.index)\n",
    "train_df_norm[['customer_id', 'product_parent', 'positive', 'pos_sim', 'neg_sim',\n",
    "        'pos_2_word_1', 'pos_2_word_2', 'pos_2_word_3', 'pos_3_word_1',\n",
    "        'pos_3_word_2', 'pos_3_word_3', 'neg_1_word_1', 'neg_1_word_2',\n",
    "        'neg_1_word_3', 'neg_2_word_1', 'neg_2_word_2', 'neg_2_word_3',\n",
    "        'neg_3_word_1', 'neg_3_word_2', 'neg_3_word_3']] = train_ids\n",
    "train_df_norm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids = test_df[['customer_id', 'product_parent', 'positive', 'pos_sim', 'neg_sim',\n",
    "        'pos_2_word_1', 'pos_2_word_2', 'pos_2_word_3', 'pos_3_word_1',\n",
    "        'pos_3_word_2', 'pos_3_word_3', 'neg_1_word_1', 'neg_1_word_2',\n",
    "        'neg_1_word_3', 'neg_2_word_1', 'neg_2_word_2', 'neg_2_word_3',\n",
    "        'neg_3_word_1', 'neg_3_word_2', 'neg_3_word_3']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>product_parent</th>\n",
       "      <th>positive</th>\n",
       "      <th>cust_review_count</th>\n",
       "      <th>cust_star_rating_mean</th>\n",
       "      <th>cust_star_rating_std</th>\n",
       "      <th>prod_review_count</th>\n",
       "      <th>prod_star_rating_mean</th>\n",
       "      <th>prod_star_rating_std</th>\n",
       "      <th>pos_sim</th>\n",
       "      <th>...</th>\n",
       "      <th>pos_3_word_3</th>\n",
       "      <th>neg_1_word_1</th>\n",
       "      <th>neg_1_word_2</th>\n",
       "      <th>neg_1_word_3</th>\n",
       "      <th>neg_2_word_1</th>\n",
       "      <th>neg_2_word_2</th>\n",
       "      <th>neg_2_word_3</th>\n",
       "      <th>neg_3_word_1</th>\n",
       "      <th>neg_3_word_2</th>\n",
       "      <th>neg_3_word_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>52449052</td>\n",
       "      <td>437083384</td>\n",
       "      <td>0</td>\n",
       "      <td>0.009662</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.656696</td>\n",
       "      <td>0.064856</td>\n",
       "      <td>0.875921</td>\n",
       "      <td>0.313757</td>\n",
       "      <td>0.010390</td>\n",
       "      <td>...</td>\n",
       "      <td>0.187860</td>\n",
       "      <td>0.083284</td>\n",
       "      <td>-0.115922</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.090283</td>\n",
       "      <td>-0.083403</td>\n",
       "      <td>0.025464</td>\n",
       "      <td>0.084633</td>\n",
       "      <td>-0.092710</td>\n",
       "      <td>0.019767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>43483672</td>\n",
       "      <td>437083384</td>\n",
       "      <td>0</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.176777</td>\n",
       "      <td>0.064856</td>\n",
       "      <td>0.875921</td>\n",
       "      <td>0.313757</td>\n",
       "      <td>0.020654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.610160</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>24034740</td>\n",
       "      <td>398163686</td>\n",
       "      <td>0</td>\n",
       "      <td>0.009662</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.564579</td>\n",
       "      <td>0.200319</td>\n",
       "      <td>0.907371</td>\n",
       "      <td>0.269515</td>\n",
       "      <td>0.014433</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.039616</td>\n",
       "      <td>0.267007</td>\n",
       "      <td>0.181326</td>\n",
       "      <td>-0.074948</td>\n",
       "      <td>0.123838</td>\n",
       "      <td>0.331554</td>\n",
       "      <td>0.001353</td>\n",
       "      <td>0.043641</td>\n",
       "      <td>0.248017</td>\n",
       "      <td>0.021189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>52088988</td>\n",
       "      <td>817183381</td>\n",
       "      <td>0</td>\n",
       "      <td>0.009662</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.433013</td>\n",
       "      <td>0.007827</td>\n",
       "      <td>0.745000</td>\n",
       "      <td>0.502494</td>\n",
       "      <td>0.021552</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039567</td>\n",
       "      <td>0.100507</td>\n",
       "      <td>0.119207</td>\n",
       "      <td>0.110014</td>\n",
       "      <td>-0.135831</td>\n",
       "      <td>-0.123986</td>\n",
       "      <td>-0.137193</td>\n",
       "      <td>-0.026096</td>\n",
       "      <td>-0.011760</td>\n",
       "      <td>-0.014463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>22725739</td>\n",
       "      <td>336891384</td>\n",
       "      <td>0</td>\n",
       "      <td>0.009662</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.774597</td>\n",
       "      <td>0.124920</td>\n",
       "      <td>0.831418</td>\n",
       "      <td>0.399799</td>\n",
       "      <td>0.013455</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.095906</td>\n",
       "      <td>0.080551</td>\n",
       "      <td>-0.044000</td>\n",
       "      <td>-0.042876</td>\n",
       "      <td>0.482972</td>\n",
       "      <td>-0.198119</td>\n",
       "      <td>-0.183380</td>\n",
       "      <td>0.234785</td>\n",
       "      <td>-0.099817</td>\n",
       "      <td>-0.117829</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    customer_id  product_parent  positive  cust_review_count  \\\n",
       "3      52449052       437083384         0           0.009662   \n",
       "5      43483672       437083384         0           0.014493   \n",
       "13     24034740       398163686         0           0.009662   \n",
       "23     52088988       817183381         0           0.009662   \n",
       "31     22725739       336891384         0           0.009662   \n",
       "\n",
       "    cust_star_rating_mean  cust_star_rating_std  prod_review_count  \\\n",
       "3                0.600000              0.656696           0.064856   \n",
       "5                0.958333              0.176777           0.064856   \n",
       "13               0.700000              0.564579           0.200319   \n",
       "23               0.750000              0.433013           0.007827   \n",
       "31               0.800000              0.774597           0.124920   \n",
       "\n",
       "    prod_star_rating_mean  prod_star_rating_std   pos_sim  ...  pos_3_word_3  \\\n",
       "3                0.875921              0.313757  0.010390  ...      0.187860   \n",
       "5                0.875921              0.313757  0.020654  ...      0.610160   \n",
       "13               0.907371              0.269515  0.014433  ...     -0.039616   \n",
       "23               0.745000              0.502494  0.021552  ...      0.039567   \n",
       "31               0.831418              0.399799  0.013455  ...     -0.095906   \n",
       "\n",
       "    neg_1_word_1  neg_1_word_2  neg_1_word_3  neg_2_word_1  neg_2_word_2  \\\n",
       "3       0.083284     -0.115922      0.023810      0.090283     -0.083403   \n",
       "5       0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "13      0.267007      0.181326     -0.074948      0.123838      0.331554   \n",
       "23      0.100507      0.119207      0.110014     -0.135831     -0.123986   \n",
       "31      0.080551     -0.044000     -0.042876      0.482972     -0.198119   \n",
       "\n",
       "    neg_2_word_3  neg_3_word_1  neg_3_word_2  neg_3_word_3  \n",
       "3       0.025464      0.084633     -0.092710      0.019767  \n",
       "5       0.000000      0.000000      0.000000      0.000000  \n",
       "13      0.001353      0.043641      0.248017      0.021189  \n",
       "23     -0.137193     -0.026096     -0.011760     -0.014463  \n",
       "31     -0.183380      0.234785     -0.099817     -0.117829  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_norm = pd.DataFrame(min_max_scaler.transform(test_df), columns=test_df.columns, index=test_df.index)\n",
    "test_df_norm[['customer_id', 'product_parent', 'positive', 'pos_sim', 'neg_sim',\n",
    "        'pos_2_word_1', 'pos_2_word_2', 'pos_2_word_3', 'pos_3_word_1',\n",
    "        'pos_3_word_2', 'pos_3_word_3', 'neg_1_word_1', 'neg_1_word_2',\n",
    "        'neg_1_word_3', 'neg_2_word_1', 'neg_2_word_2', 'neg_2_word_3',\n",
    "        'neg_3_word_1', 'neg_3_word_2', 'neg_3_word_3']] = test_ids\n",
    "test_df_norm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = train_df.drop('positive', axis=1), train_df['positive']\n",
    "test_x, test_y = test_df.drop('positive', axis=1), test_df['positive']\n",
    "train_x_norm = train_df_norm.drop('positive', axis=1)\n",
    "test_x_norm = test_df.drop('positive', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>product_parent</th>\n",
       "      <th>positive</th>\n",
       "      <th>cust_review_count</th>\n",
       "      <th>cust_star_rating_mean</th>\n",
       "      <th>cust_star_rating_std</th>\n",
       "      <th>prod_review_count</th>\n",
       "      <th>prod_star_rating_mean</th>\n",
       "      <th>prod_star_rating_std</th>\n",
       "      <th>pos_sim</th>\n",
       "      <th>...</th>\n",
       "      <th>pos_3_word_3</th>\n",
       "      <th>neg_1_word_1</th>\n",
       "      <th>neg_1_word_2</th>\n",
       "      <th>neg_1_word_3</th>\n",
       "      <th>neg_2_word_1</th>\n",
       "      <th>neg_2_word_2</th>\n",
       "      <th>neg_2_word_3</th>\n",
       "      <th>neg_3_word_1</th>\n",
       "      <th>neg_3_word_2</th>\n",
       "      <th>neg_3_word_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [customer_id, product_parent, positive, cust_review_count, cust_star_rating_mean, cust_star_rating_std, prod_review_count, prod_star_rating_mean, prod_star_rating_std, pos_sim, neg_sim, pos_1_word_1, pos_1_word_2, pos_1_word_3, pos_2_word_1, pos_2_word_2, pos_2_word_3, pos_3_word_1, pos_3_word_2, pos_3_word_3, neg_1_word_1, neg_1_word_2, neg_1_word_3, neg_2_word_1, neg_2_word_2, neg_2_word_3, neg_3_word_1, neg_3_word_2, neg_3_word_3]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 29 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_NaN = train_df_norm.isnull()\n",
    "row_has_NaN = is_NaN.any(axis=1)\n",
    "train_df_norm[row_has_NaN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
       "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "                           max_features=None, max_leaf_nodes=None,\n",
       "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                           min_samples_leaf=1, min_samples_split=2,\n",
       "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                           n_iter_no_change=None, presort='deprecated',\n",
       "                           random_state=None, subsample=1.0, tol=0.0001,\n",
       "                           validation_fraction=0.1, verbose=0,\n",
       "                           warm_start=False)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gb = GradientBoostingClassifier()\n",
    "gb.fit(train_x_norm.drop(['customer_id', 'product_parent'], axis=1), train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.515952980688497"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb.score(test_x_norm.drop(['customer_id', 'product_parent'], axis=1), test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=1000)\n",
    "rf.fit(train_x.drop(['customer_id', 'product_parent'], axis=1), train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7107472712006717"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.score(test_x.drop(['customer_id', 'product_parent'], axis=1), test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 26 artists>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "imp_tuples = sorted(zip(map(lambda x: round(x, 4), rf.feature_importances_), \n",
    "                 train_x.drop(['customer_id', 'product_parent'], axis=1).columns), reverse=False)\n",
    "imp = [t[0] for t in imp_tuples]\n",
    "labl = [t[1] for t in imp_tuples]\n",
    "plt.barh(labl, imp, linewidth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=25, p=2,\n",
       "                     weights='distance')"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=25, weights='distance')\n",
    "knn.fit(train_x_norm.drop(['customer_id', 'product_parent'], axis=1), train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5046179680940386"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.score(test_x_norm.drop(['customer_id', 'product_parent'], axis=1), test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(train_x_norm.drop(['customer_id', 'product_parent'], axis=1), train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6043940666106913"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb.score(test_x_norm.drop(['customer_id', 'product_parent'], axis=1), test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 6721 0.42443953268077045\n",
      "Pos: 3928 0.24805809914745816\n",
      "Star Correct: 6721 0.42443953268077045\n",
      "True Positive: 3538 0.22342911272497631\n",
      "True Negative: 3183 0.20101041995579413\n",
      "False Positive: 390 0.024628986422481843\n",
      "False Negative: 8724 0.5509314808967477\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "pos_count = 0\n",
    "star_cor = 0\n",
    "matrix = [0, 0, 0, 0]\n",
    "preds = rf.predict(test_x.drop(['customer_id', 'product_parent'], axis=1))\n",
    "for i, row in test_df.iterrows():\n",
    "    if (preds[i] > 0):\n",
    "        pos_count += 1\n",
    "    if (preds[i] == row.positive):\n",
    "        star_cor += 1\n",
    "    if (preds[i] > 0 and row.positive > 0):\n",
    "        correct += 1\n",
    "        matrix[0] += 1\n",
    "    elif (preds[i] <= 0 and row.positive <= 0):\n",
    "        correct += 1\n",
    "        matrix[1] += 1\n",
    "    elif (preds[i] > 0 and row.positive <= 0):\n",
    "        matrix[2] += 1\n",
    "    elif (preds[i] <= 0 and row.positive > 0):\n",
    "        matrix[3] += 1\n",
    "\n",
    "print('Correct:', correct, correct / len(test_df))\n",
    "print('Pos:', pos_count, pos_count / len(test_df))\n",
    "print('Star Correct:', star_cor, star_cor / len(test_df))\n",
    "print('True Positive:', matrix[0], matrix[0] / len(test_df))\n",
    "print('True Negative:', matrix[1], matrix[1] / len(test_df))\n",
    "print('False Positive:', matrix[2], matrix[2] / len(test_df))\n",
    "print('False Negative:', matrix[3], matrix[3] / len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical\n",
    "import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_train_x = train_x_norm.drop(['customer_id', 'product_parent'], axis=1)\n",
    "nn_test_x = test_x_norm.drop(['customer_id', 'product_parent'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19062, 26)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(nn_train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_cat = to_categorical(train_y)\n",
    "test_y_cat = to_categorical(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 1024)              27648     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 3,505,921\n",
      "Trainable params: 3,505,921\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "network = models.Sequential()\n",
    "network.add(layers.Dense(1024, activation='relu', input_shape=(26,)))\n",
    "network.add(layers.Dropout(0.5))\n",
    "network.add(layers.Dense(1024, activation='relu', input_shape=(1024,)))\n",
    "network.add(layers.Dense(1024, activation='relu', input_shape=(1024,)))\n",
    "network.add(layers.Dense(512, activation='relu', input_shape=(1024,)))\n",
    "network.add(layers.Dropout(0.5))\n",
    "network.add(layers.Dense(512, activation='relu', input_shape=(512,)))\n",
    "network.add(layers.Dense(512, activation='relu', input_shape=(512,)))\n",
    "network.add(layers.Dense(256, activation='relu', input_shape=(512,)))\n",
    "network.add(layers.Dropout(0.5))\n",
    "network.add(layers.Dense(256, activation='relu', input_shape=(256,)))\n",
    "network.add(layers.Dense(256, activation='relu', input_shape=(256,)))\n",
    "network.add(layers.Dense(128, activation='relu', input_shape=(256,)))\n",
    "network.add(layers.Dropout(0.5))\n",
    "network.add(layers.Dense(128, activation='relu', input_shape=(128,)))\n",
    "network.add(layers.Dense(128, activation='relu', input_shape=(128,)))\n",
    "network.add(layers.Dense(1, activation='sigmoid', input_shape=(128,)))\n",
    "network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19062 samples, validate on 7146 samples\n",
      "Epoch 1/500\n",
      "19062/19062 [==============================] - 2s 87us/step - loss: 0.6331 - accuracy: 0.6470 - val_loss: 197.7960 - val_accuracy: 0.5056\n",
      "Epoch 2/500\n",
      "19062/19062 [==============================] - 1s 58us/step - loss: 0.5553 - accuracy: 0.7379 - val_loss: 105.6197 - val_accuracy: 0.5003\n",
      "Epoch 3/500\n",
      "19062/19062 [==============================] - 1s 60us/step - loss: 0.5378 - accuracy: 0.7507 - val_loss: 92.3173 - val_accuracy: 0.5000\n",
      "Epoch 4/500\n",
      "19062/19062 [==============================] - 1s 56us/step - loss: 0.5300 - accuracy: 0.7523 - val_loss: 79.9236 - val_accuracy: 0.5000\n",
      "Epoch 5/500\n",
      "19062/19062 [==============================] - 1s 57us/step - loss: 0.5166 - accuracy: 0.7635 - val_loss: 189.4985 - val_accuracy: 0.5000\n",
      "Epoch 6/500\n",
      "19062/19062 [==============================] - 1s 56us/step - loss: 0.5074 - accuracy: 0.7684 - val_loss: 134.7213 - val_accuracy: 0.5000\n",
      "Epoch 7/500\n",
      "19062/19062 [==============================] - 1s 56us/step - loss: 0.5025 - accuracy: 0.7711 - val_loss: 174.4816 - val_accuracy: 0.5000\n",
      "Epoch 8/500\n",
      "19062/19062 [==============================] - ETA: 0s - loss: 0.5019 - accuracy: 0.76 - 1s 57us/step - loss: 0.5023 - accuracy: 0.7684 - val_loss: 77.9005 - val_accuracy: 0.5000\n",
      "Epoch 9/500\n",
      "19062/19062 [==============================] - 1s 56us/step - loss: 0.4975 - accuracy: 0.7736 - val_loss: 284.3118 - val_accuracy: 0.5000\n",
      "Epoch 10/500\n",
      "19062/19062 [==============================] - 1s 56us/step - loss: 0.5255 - accuracy: 0.7775 - val_loss: 363.3837 - val_accuracy: 0.5000\n",
      "Epoch 11/500\n",
      "19062/19062 [==============================] - 1s 56us/step - loss: 0.4924 - accuracy: 0.7777 - val_loss: 5144.8232 - val_accuracy: 0.5000\n",
      "Epoch 12/500\n",
      "19062/19062 [==============================] - 1s 56us/step - loss: 0.4880 - accuracy: 0.7807 - val_loss: 4170.6344 - val_accuracy: 0.5000\n",
      "Epoch 13/500\n",
      "19062/19062 [==============================] - 1s 57us/step - loss: 0.4960 - accuracy: 0.7772 - val_loss: 16776.6988 - val_accuracy: 0.5000\n",
      "Epoch 14/500\n",
      "19062/19062 [==============================] - 1s 57us/step - loss: 0.4875 - accuracy: 0.7790 - val_loss: 327557.6583 - val_accuracy: 0.5000\n",
      "Epoch 15/500\n",
      "19062/19062 [==============================] - 1s 55us/step - loss: 0.5444 - accuracy: 0.7807 - val_loss: 23600.1705 - val_accuracy: 0.5003\n",
      "Epoch 16/500\n",
      "19062/19062 [==============================] - 1s 56us/step - loss: 0.6071 - accuracy: 0.7811 - val_loss: 26423.7829 - val_accuracy: 0.5003\n",
      "Epoch 17/500\n",
      "19062/19062 [==============================] - 1s 56us/step - loss: 0.4845 - accuracy: 0.7792 - val_loss: 49566.7282 - val_accuracy: 0.5001\n",
      "Epoch 18/500\n",
      "19062/19062 [==============================] - 1s 56us/step - loss: 0.4823 - accuracy: 0.7829 - val_loss: 22162.8549 - val_accuracy: 0.4061\n",
      "Epoch 19/500\n",
      "19062/19062 [==============================] - 1s 57us/step - loss: 0.5982 - accuracy: 0.7819 - val_loss: 28481.3290 - val_accuracy: 0.5006\n",
      "Epoch 20/500\n",
      "19062/19062 [==============================] - 1s 56us/step - loss: 0.4864 - accuracy: 0.7813 - val_loss: 63127.7761 - val_accuracy: 0.4299\n",
      "Epoch 21/500\n",
      "19062/19062 [==============================] - 1s 56us/step - loss: 0.4807 - accuracy: 0.7813 - val_loss: 102337.6135 - val_accuracy: 0.4708\n",
      "Epoch 22/500\n",
      "19062/19062 [==============================] - 1s 56us/step - loss: 0.4865 - accuracy: 0.7848 - val_loss: 111964.3829 - val_accuracy: 0.4152\n",
      "Epoch 23/500\n",
      "19062/19062 [==============================] - 1s 56us/step - loss: 0.4839 - accuracy: 0.7843 - val_loss: 106045.4934 - val_accuracy: 0.4843\n",
      "Epoch 24/500\n",
      "19062/19062 [==============================] - 1s 55us/step - loss: 0.4924 - accuracy: 0.7849 - val_loss: 62041.9029 - val_accuracy: 0.4386\n",
      "Epoch 25/500\n",
      "19062/19062 [==============================] - 1s 56us/step - loss: 0.4801 - accuracy: 0.7853 - val_loss: 104485.6685 - val_accuracy: 0.4516\n",
      "Epoch 26/500\n",
      "19062/19062 [==============================] - 1s 55us/step - loss: 0.4834 - accuracy: 0.7867 - val_loss: 58748.9881 - val_accuracy: 0.4426\n",
      "Epoch 27/500\n",
      "19062/19062 [==============================] - 1s 55us/step - loss: 0.4795 - accuracy: 0.7861 - val_loss: 62069.8833 - val_accuracy: 0.4912\n",
      "Epoch 28/500\n",
      "19062/19062 [==============================] - 1s 56us/step - loss: 0.4916 - accuracy: 0.7884 - val_loss: 166954.7382 - val_accuracy: 0.4254\n",
      "Epoch 29/500\n",
      "19062/19062 [==============================] - 1s 55us/step - loss: 0.5785 - accuracy: 0.7872 - val_loss: 28563.9341 - val_accuracy: 0.3913\n",
      "Epoch 30/500\n",
      "19062/19062 [==============================] - 1s 56us/step - loss: 0.4764 - accuracy: 0.7870 - val_loss: 56149.5872 - val_accuracy: 0.4803\n",
      "Epoch 31/500\n",
      "19062/19062 [==============================] - 1s 55us/step - loss: 0.4849 - accuracy: 0.7865 - val_loss: 66754.9822 - val_accuracy: 0.4685\n",
      "Epoch 32/500\n",
      "19062/19062 [==============================] - 1s 55us/step - loss: 0.4876 - accuracy: 0.7892 - val_loss: 47728.0565 - val_accuracy: 0.4401\n",
      "Epoch 33/500\n",
      "19062/19062 [==============================] - 1s 56us/step - loss: 0.4807 - accuracy: 0.7869 - val_loss: 56408.0136 - val_accuracy: 0.4635\n",
      "Epoch 34/500\n",
      "19062/19062 [==============================] - 1s 56us/step - loss: 0.4790 - accuracy: 0.7884 - val_loss: 124556.4476 - val_accuracy: 0.4597\n",
      "Epoch 35/500\n",
      "19062/19062 [==============================] - 1s 56us/step - loss: 0.4842 - accuracy: 0.7901 - val_loss: 193052.3978 - val_accuracy: 0.4787\n",
      "Epoch 36/500\n",
      "19062/19062 [==============================] - 1s 56us/step - loss: 0.5648 - accuracy: 0.7902 - val_loss: 65136.0558 - val_accuracy: 0.4691\n",
      "Epoch 37/500\n",
      "19062/19062 [==============================] - 1s 55us/step - loss: 0.4731 - accuracy: 0.7893 - val_loss: 48284.4837 - val_accuracy: 0.4759\n",
      "Epoch 38/500\n",
      "19062/19062 [==============================] - 1s 55us/step - loss: 0.4775 - accuracy: 0.7896 - val_loss: 45535.4179 - val_accuracy: 0.4684\n",
      "Epoch 39/500\n",
      "19062/19062 [==============================] - 1s 55us/step - loss: 0.4829 - accuracy: 0.7893 - val_loss: 88304.5668 - val_accuracy: 0.4653\n",
      "Epoch 40/500\n",
      "19062/19062 [==============================] - 1s 55us/step - loss: 0.5097 - accuracy: 0.7915 - val_loss: 53185.7502 - val_accuracy: 0.4772\n",
      "Epoch 41/500\n",
      "19062/19062 [==============================] - 1s 55us/step - loss: 0.4751 - accuracy: 0.7893 - val_loss: 88303.6488 - val_accuracy: 0.4846\n",
      "Epoch 42/500\n",
      "19062/19062 [==============================] - 1s 56us/step - loss: 0.4762 - accuracy: 0.7934 - val_loss: 112508.2651 - val_accuracy: 0.4867\n",
      "Epoch 43/500\n",
      "19062/19062 [==============================] - 1s 55us/step - loss: 0.4724 - accuracy: 0.7922 - val_loss: 73202.4516 - val_accuracy: 0.4924\n",
      "Epoch 44/500\n",
      "19062/19062 [==============================] - 1s 56us/step - loss: 0.4842 - accuracy: 0.7892 - val_loss: 86740.3375 - val_accuracy: 0.4766\n",
      "Epoch 45/500\n",
      "19062/19062 [==============================] - 1s 56us/step - loss: 0.4731 - accuracy: 0.7909 - val_loss: 47228.2204 - val_accuracy: 0.4797\n",
      "Epoch 46/500\n",
      "19062/19062 [==============================] - 1s 55us/step - loss: 0.4742 - accuracy: 0.7907 - val_loss: 23293.5665 - val_accuracy: 0.4720\n",
      "Epoch 47/500\n",
      "19062/19062 [==============================] - 1s 56us/step - loss: 0.4704 - accuracy: 0.7918 - val_loss: 81875.5861 - val_accuracy: 0.4808\n",
      "Epoch 48/500\n",
      "19062/19062 [==============================] - 1s 57us/step - loss: 0.4925 - accuracy: 0.7913 - val_loss: 191313.7262 - val_accuracy: 0.4924\n",
      "Epoch 49/500\n",
      "19062/19062 [==============================] - 1s 57us/step - loss: 0.4749 - accuracy: 0.7916 - val_loss: 25920.1294 - val_accuracy: 0.4826\n",
      "Epoch 50/500\n",
      "19062/19062 [==============================] - 1s 55us/step - loss: 0.4727 - accuracy: 0.7903 - val_loss: 71407.5057 - val_accuracy: 0.4863\n",
      "Epoch 51/500\n",
      "19062/19062 [==============================] - 1s 57us/step - loss: 0.4714 - accuracy: 0.7926 - val_loss: 456853.2629 - val_accuracy: 0.4866\n",
      "Epoch 52/500\n",
      "19062/19062 [==============================] - 1s 58us/step - loss: 0.5314 - accuracy: 0.7954 - val_loss: 111416.7494 - val_accuracy: 0.4821\n",
      "Epoch 53/500\n",
      "19062/19062 [==============================] - 1s 58us/step - loss: 0.4705 - accuracy: 0.7944 - val_loss: 39183.2290 - val_accuracy: 0.4811\n",
      "Epoch 54/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19062/19062 [==============================] - 1s 56us/step - loss: 0.4667 - accuracy: 0.7941 - val_loss: 112920.7192 - val_accuracy: 0.4908\n",
      "Epoch 55/500\n",
      "19062/19062 [==============================] - 1s 60us/step - loss: 0.4748 - accuracy: 0.7930 - val_loss: 22312.0539 - val_accuracy: 0.5004\n",
      "Epoch 56/500\n",
      "19062/19062 [==============================] - 1s 58us/step - loss: 0.4883 - accuracy: 0.7922 - val_loss: 50663.9759 - val_accuracy: 0.4902\n",
      "Epoch 57/500\n",
      "19062/19062 [==============================] - 1s 57us/step - loss: 0.4682 - accuracy: 0.7904 - val_loss: 40027.8443 - val_accuracy: 0.4874\n",
      "Epoch 58/500\n",
      "19062/19062 [==============================] - 1s 56us/step - loss: 0.4786 - accuracy: 0.7918 - val_loss: 27795.2639 - val_accuracy: 0.4880\n",
      "Epoch 59/500\n",
      "19062/19062 [==============================] - 1s 57us/step - loss: 0.4715 - accuracy: 0.7933 - val_loss: 66334.6293 - val_accuracy: 0.4915\n",
      "Epoch 60/500\n",
      "19062/19062 [==============================] - 1s 56us/step - loss: 0.4698 - accuracy: 0.7914 - val_loss: 54267.3683 - val_accuracy: 0.4853\n",
      "Epoch 61/500\n",
      "19062/19062 [==============================] - 1s 56us/step - loss: 0.4721 - accuracy: 0.7936 - val_loss: 26958.9987 - val_accuracy: 0.4859\n",
      "Epoch 62/500\n",
      "19062/19062 [==============================] - 1s 55us/step - loss: 0.4695 - accuracy: 0.7911 - val_loss: 13792.0597 - val_accuracy: 0.4954\n",
      "Epoch 63/500\n",
      "19062/19062 [==============================] - 1s 56us/step - loss: 0.5238 - accuracy: 0.7922 - val_loss: 167096.3961 - val_accuracy: 0.4889\n",
      "Epoch 64/500\n",
      "19062/19062 [==============================] - 1s 56us/step - loss: 0.5331 - accuracy: 0.7949 - val_loss: 302951.1060 - val_accuracy: 0.4934\n",
      "Epoch 65/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.4778 - accuracy: 0.7933 - val_loss: 61192.3560 - val_accuracy: 0.4836\n",
      "Epoch 66/500\n",
      "19062/19062 [==============================] - 1s 55us/step - loss: 0.4721 - accuracy: 0.7903 - val_loss: 61278.5954 - val_accuracy: 0.4868\n",
      "Epoch 67/500\n",
      "19062/19062 [==============================] - 1s 55us/step - loss: 0.4809 - accuracy: 0.7933 - val_loss: 95206.9129 - val_accuracy: 0.4764\n",
      "Epoch 68/500\n",
      "19062/19062 [==============================] - 1s 55us/step - loss: 0.4633 - accuracy: 0.7954 - val_loss: 151497.7440 - val_accuracy: 0.4889\n",
      "Epoch 69/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4746 - accuracy: 0.7939 - val_loss: 38568.0105 - val_accuracy: 0.4958\n",
      "Epoch 70/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.5159 - accuracy: 0.7947 - val_loss: 116112.2441 - val_accuracy: 0.4906\n",
      "Epoch 71/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4727 - accuracy: 0.7958 - val_loss: 341740.0625 - val_accuracy: 0.4969\n",
      "Epoch 72/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4714 - accuracy: 0.7925 - val_loss: 36943.9662 - val_accuracy: 0.4982\n",
      "Epoch 73/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4697 - accuracy: 0.7945 - val_loss: 65752.1872 - val_accuracy: 0.4927\n",
      "Epoch 74/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4761 - accuracy: 0.7951 - val_loss: 106843.5922 - val_accuracy: 0.4894\n",
      "Epoch 75/500\n",
      "19062/19062 [==============================] - 1s 57us/step - loss: 0.4733 - accuracy: 0.7953 - val_loss: 91768.1137 - val_accuracy: 0.4958\n",
      "Epoch 76/500\n",
      "19062/19062 [==============================] - 1s 57us/step - loss: 0.4771 - accuracy: 0.7955 - val_loss: 393307.3811 - val_accuracy: 0.4936\n",
      "Epoch 77/500\n",
      "19062/19062 [==============================] - 1s 55us/step - loss: 0.4798 - accuracy: 0.7938 - val_loss: 34776.2449 - val_accuracy: 0.4926\n",
      "Epoch 78/500\n",
      "19062/19062 [==============================] - 1s 57us/step - loss: 0.4650 - accuracy: 0.7934 - val_loss: 94198.8983 - val_accuracy: 0.4895\n",
      "Epoch 79/500\n",
      "19062/19062 [==============================] - 1s 57us/step - loss: 0.4796 - accuracy: 0.7946 - val_loss: 151954.9253 - val_accuracy: 0.4986\n",
      "Epoch 80/500\n",
      "19062/19062 [==============================] - 1s 55us/step - loss: 0.4726 - accuracy: 0.7937 - val_loss: 24091.2838 - val_accuracy: 0.4943\n",
      "Epoch 81/500\n",
      "19062/19062 [==============================] - 1s 55us/step - loss: 0.4822 - accuracy: 0.7930 - val_loss: 178931.3287 - val_accuracy: 0.4923\n",
      "Epoch 82/500\n",
      "19062/19062 [==============================] - 1s 56us/step - loss: 0.4788 - accuracy: 0.7948 - val_loss: 35630.3936 - val_accuracy: 0.4962\n",
      "Epoch 83/500\n",
      "19062/19062 [==============================] - 1s 55us/step - loss: 0.4764 - accuracy: 0.7954 - val_loss: 236801.9331 - val_accuracy: 0.4965\n",
      "Epoch 84/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.4670 - accuracy: 0.7967 - val_loss: 81934.7871 - val_accuracy: 0.4937\n",
      "Epoch 85/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4668 - accuracy: 0.7949 - val_loss: 103851.0633 - val_accuracy: 0.4934\n",
      "Epoch 86/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4645 - accuracy: 0.7954 - val_loss: 193862.1165 - val_accuracy: 0.4913\n",
      "Epoch 87/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.4734 - accuracy: 0.7945 - val_loss: 149395.8819 - val_accuracy: 0.4943\n",
      "Epoch 88/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 1.4838 - accuracy: 0.7993 - val_loss: 185097.2013 - val_accuracy: 0.4923\n",
      "Epoch 89/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4623 - accuracy: 0.7988 - val_loss: 83078.4579 - val_accuracy: 0.4955\n",
      "Epoch 90/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.4694 - accuracy: 0.7976 - val_loss: 383322.1733 - val_accuracy: 0.4938\n",
      "Epoch 91/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4600 - accuracy: 0.7976 - val_loss: 35316.1413 - val_accuracy: 0.5090\n",
      "Epoch 92/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.8236 - accuracy: 0.7987 - val_loss: 134406.9454 - val_accuracy: 0.4937\n",
      "Epoch 93/500\n",
      "19062/19062 [==============================] - 1s 56us/step - loss: 0.4660 - accuracy: 0.7981 - val_loss: 416194.2845 - val_accuracy: 0.4968\n",
      "Epoch 94/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4913 - accuracy: 0.8003 - val_loss: 1172643.9686 - val_accuracy: 0.4969\n",
      "Epoch 95/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4632 - accuracy: 0.7982 - val_loss: 495562.8332 - val_accuracy: 0.4962\n",
      "Epoch 96/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4955 - accuracy: 0.7982 - val_loss: 832628.0134 - val_accuracy: 0.4971\n",
      "Epoch 97/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4707 - accuracy: 0.7962 - val_loss: 165994.4373 - val_accuracy: 0.4959\n",
      "Epoch 98/500\n",
      "19062/19062 [==============================] - 1s 52us/step - loss: 0.4637 - accuracy: 0.7978 - val_loss: 47620.5253 - val_accuracy: 0.4994\n",
      "Epoch 99/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4866 - accuracy: 0.7984 - val_loss: 366685.5892 - val_accuracy: 0.4973\n",
      "Epoch 100/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4641 - accuracy: 0.7973 - val_loss: 2416875.6869 - val_accuracy: 0.4976\n",
      "Epoch 101/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4673 - accuracy: 0.7978 - val_loss: 7548748.1702 - val_accuracy: 0.4997\n",
      "Epoch 102/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.4937 - accuracy: 0.7976 - val_loss: 616752.1960 - val_accuracy: 0.4973\n",
      "Epoch 103/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 1.3325 - accuracy: 0.7985 - val_loss: 902545.8602 - val_accuracy: 0.4972\n",
      "Epoch 104/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4606 - accuracy: 0.7988 - val_loss: 1013387.5053 - val_accuracy: 0.4997\n",
      "Epoch 105/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.5168 - accuracy: 0.7956 - val_loss: 597076.6286 - val_accuracy: 0.4996\n",
      "Epoch 106/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4821 - accuracy: 0.8009 - val_loss: 116128.8712 - val_accuracy: 0.5003\n",
      "Epoch 107/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.6922 - accuracy: 0.7980 - val_loss: 260862.0169 - val_accuracy: 0.4993\n",
      "Epoch 108/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4643 - accuracy: 0.7990 - val_loss: 1072207.8466 - val_accuracy: 0.4999\n",
      "Epoch 109/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.5088 - accuracy: 0.7986 - val_loss: 247775.7063 - val_accuracy: 0.5001\n",
      "Epoch 110/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4774 - accuracy: 0.7981 - val_loss: 325259.2765 - val_accuracy: 0.4959\n",
      "Epoch 111/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.4739 - accuracy: 0.7995 - val_loss: 187586.8417 - val_accuracy: 0.4994\n",
      "Epoch 112/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4650 - accuracy: 0.7969 - val_loss: 158848.1717 - val_accuracy: 0.4999\n",
      "Epoch 113/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.5030 - accuracy: 0.7972 - val_loss: 52671.1880 - val_accuracy: 0.4994\n",
      "Epoch 114/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.6177 - accuracy: 0.7985 - val_loss: 464918.0327 - val_accuracy: 0.5001\n",
      "Epoch 115/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.6428 - accuracy: 0.8001 - val_loss: 1251083.2542 - val_accuracy: 0.4999\n",
      "Epoch 116/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.4636 - accuracy: 0.7999 - val_loss: 381369.8597 - val_accuracy: 0.4993\n",
      "Epoch 117/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.5157 - accuracy: 0.7982 - val_loss: 845609.8039 - val_accuracy: 0.4990\n",
      "Epoch 118/500\n",
      "19062/19062 [==============================] - 1s 52us/step - loss: 0.5072 - accuracy: 0.7975 - val_loss: 456673.3780 - val_accuracy: 0.4996\n",
      "Epoch 119/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.4833 - accuracy: 0.7986 - val_loss: 103843.1796 - val_accuracy: 0.5000\n",
      "Epoch 120/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.4712 - accuracy: 0.7986 - val_loss: 790301.2247 - val_accuracy: 0.4993\n",
      "Epoch 121/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4776 - accuracy: 0.8000 - val_loss: 307452.6315 - val_accuracy: 0.5000\n",
      "Epoch 122/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.9020 - accuracy: 0.7997 - val_loss: 200391.6326 - val_accuracy: 0.4992\n",
      "Epoch 123/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4642 - accuracy: 0.8001 - val_loss: 1105079.0718 - val_accuracy: 0.4997\n",
      "Epoch 124/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.5182 - accuracy: 0.8000 - val_loss: 446510.6607 - val_accuracy: 0.4999\n",
      "Epoch 125/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.4639 - accuracy: 0.8013 - val_loss: 646305.9616 - val_accuracy: 0.4999\n",
      "Epoch 126/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.4864 - accuracy: 0.7986 - val_loss: 488100.7423 - val_accuracy: 0.4997\n",
      "Epoch 127/500\n",
      "19062/19062 [==============================] - 1s 55us/step - loss: 0.4685 - accuracy: 0.7990 - val_loss: 169294.7184 - val_accuracy: 0.4997\n",
      "Epoch 128/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4759 - accuracy: 0.7989 - val_loss: 252323.4462 - val_accuracy: 0.4994\n",
      "Epoch 129/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.4669 - accuracy: 0.8004 - val_loss: 220656.0823 - val_accuracy: 0.4996\n",
      "Epoch 130/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4719 - accuracy: 0.8004 - val_loss: 368165.6679 - val_accuracy: 0.5000\n",
      "Epoch 131/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.4674 - accuracy: 0.7988 - val_loss: 103565.6125 - val_accuracy: 0.5000\n",
      "Epoch 132/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4613 - accuracy: 0.7984 - val_loss: 181350.0892 - val_accuracy: 0.5000\n",
      "Epoch 133/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.5202 - accuracy: 0.7997 - val_loss: 10295.7167 - val_accuracy: 0.4999\n",
      "Epoch 134/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.6226 - accuracy: 0.8029 - val_loss: 354949.0702 - val_accuracy: 0.4993\n",
      "Epoch 135/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4579 - accuracy: 0.8004 - val_loss: 160322.6251 - val_accuracy: 0.5001\n",
      "Epoch 136/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4680 - accuracy: 0.8029 - val_loss: 272184.9441 - val_accuracy: 0.4999\n",
      "Epoch 137/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.4706 - accuracy: 0.8013 - val_loss: 536319.7197 - val_accuracy: 0.5266\n",
      "Epoch 138/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4619 - accuracy: 0.8010 - val_loss: 82466.5588 - val_accuracy: 0.4999\n",
      "Epoch 139/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4648 - accuracy: 0.8027 - val_loss: 408793.6702 - val_accuracy: 0.4997\n",
      "Epoch 140/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 1.7918 - accuracy: 0.7995 - val_loss: 48373.3054 - val_accuracy: 0.5367\n",
      "Epoch 141/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4650 - accuracy: 0.8003 - val_loss: 570245.6534 - val_accuracy: 0.4999\n",
      "Epoch 142/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.4653 - accuracy: 0.7958 - val_loss: 211894.9037 - val_accuracy: 0.5000\n",
      "Epoch 143/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.9577 - accuracy: 0.7994 - val_loss: 449175.6235 - val_accuracy: 0.4999\n",
      "Epoch 144/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 22.8321 - accuracy: 0.8010 - val_loss: 459807.3363 - val_accuracy: 0.4999\n",
      "Epoch 145/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 1.9603 - accuracy: 0.8004 - val_loss: 287020.7695 - val_accuracy: 0.5000\n",
      "Epoch 146/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.4607 - accuracy: 0.8044 - val_loss: 286474.2621 - val_accuracy: 0.5001\n",
      "Epoch 147/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4653 - accuracy: 0.7983 - val_loss: 182862.8329 - val_accuracy: 0.5001\n",
      "Epoch 148/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.5974 - accuracy: 0.7999 - val_loss: 598808.1992 - val_accuracy: 0.5000\n",
      "Epoch 149/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.5325 - accuracy: 0.7988 - val_loss: 158381.3493 - val_accuracy: 0.5001\n",
      "Epoch 150/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4714 - accuracy: 0.7995 - val_loss: 316767.2142 - val_accuracy: 0.5001\n",
      "Epoch 151/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.5066 - accuracy: 0.8000 - val_loss: 88071.7336 - val_accuracy: 0.5001\n",
      "Epoch 152/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.5779 - accuracy: 0.8028 - val_loss: 108396.3118 - val_accuracy: 0.5001\n",
      "Epoch 153/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.5217 - accuracy: 0.8007 - val_loss: 67657.8983 - val_accuracy: 0.5001\n",
      "Epoch 154/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4892 - accuracy: 0.8001 - val_loss: 104407.1600 - val_accuracy: 0.5001\n",
      "Epoch 155/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.7695 - accuracy: 0.8033 - val_loss: 128670.9904 - val_accuracy: 0.5001\n",
      "Epoch 156/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.5090 - accuracy: 0.8020 - val_loss: 361854.2404 - val_accuracy: 0.5001\n",
      "Epoch 157/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4651 - accuracy: 0.8004 - val_loss: 357968.5271 - val_accuracy: 0.5000\n",
      "Epoch 158/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.4602 - accuracy: 0.7988 - val_loss: 266072.3415 - val_accuracy: 0.5001\n",
      "Epoch 159/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.6005 - accuracy: 0.8015 - val_loss: 164964.8186 - val_accuracy: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 160/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4822 - accuracy: 0.7979 - val_loss: 128382.1288 - val_accuracy: 0.5001\n",
      "Epoch 161/500\n",
      "19062/19062 [==============================] - 1s 55us/step - loss: 0.4842 - accuracy: 0.7871 - val_loss: 70556.6499 - val_accuracy: 0.5001\n",
      "Epoch 162/500\n",
      "19062/19062 [==============================] - 1s 58us/step - loss: 0.7448 - accuracy: 0.8025 - val_loss: 90243.3971 - val_accuracy: 0.5001\n",
      "Epoch 163/500\n",
      "19062/19062 [==============================] - 1s 59us/step - loss: 0.4877 - accuracy: 0.7995 - val_loss: 138786.6633 - val_accuracy: 0.5001\n",
      "Epoch 164/500\n",
      "19062/19062 [==============================] - 1s 56us/step - loss: 0.8843 - accuracy: 0.7996 - val_loss: 413481.4429 - val_accuracy: 0.5001\n",
      "Epoch 165/500\n",
      "19062/19062 [==============================] - 1s 57us/step - loss: 0.4882 - accuracy: 0.8021 - val_loss: 159672.9523 - val_accuracy: 0.5001\n",
      "Epoch 166/500\n",
      "19062/19062 [==============================] - 1s 55us/step - loss: 0.4748 - accuracy: 0.7944 - val_loss: 79153.8715 - val_accuracy: 0.5000\n",
      "Epoch 167/500\n",
      "19062/19062 [==============================] - 1s 56us/step - loss: 0.4687 - accuracy: 0.8010 - val_loss: 178348.9496 - val_accuracy: 0.5000\n",
      "Epoch 168/500\n",
      "19062/19062 [==============================] - 1s 55us/step - loss: 0.8800 - accuracy: 0.8022 - val_loss: 163253.5302 - val_accuracy: 0.5000\n",
      "Epoch 169/500\n",
      "19062/19062 [==============================] - 1s 55us/step - loss: 0.4781 - accuracy: 0.7990 - val_loss: 90166.5335 - val_accuracy: 0.4640\n",
      "Epoch 170/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.4734 - accuracy: 0.7994 - val_loss: 253222.0936 - val_accuracy: 0.5001\n",
      "Epoch 171/500\n",
      "19062/19062 [==============================] - 1s 55us/step - loss: 0.5797 - accuracy: 0.7993 - val_loss: 209568.4048 - val_accuracy: 0.5001\n",
      "Epoch 172/500\n",
      "19062/19062 [==============================] - 1s 56us/step - loss: 0.4598 - accuracy: 0.8025 - val_loss: 19647.8846 - val_accuracy: 0.5001\n",
      "Epoch 173/500\n",
      "19062/19062 [==============================] - 1s 55us/step - loss: 0.4762 - accuracy: 0.7970 - val_loss: 43938.0360 - val_accuracy: 0.5000\n",
      "Epoch 174/500\n",
      "19062/19062 [==============================] - 1s 55us/step - loss: 0.4718 - accuracy: 0.8027 - val_loss: 80673.5160 - val_accuracy: 0.5001\n",
      "Epoch 175/500\n",
      "19062/19062 [==============================] - 1s 56us/step - loss: 0.4709 - accuracy: 0.7994 - val_loss: 142314.2180 - val_accuracy: 0.5257\n",
      "Epoch 176/500\n",
      "19062/19062 [==============================] - 1s 55us/step - loss: 0.5135 - accuracy: 0.8014 - val_loss: 457568.7801 - val_accuracy: 0.4232\n",
      "Epoch 177/500\n",
      "19062/19062 [==============================] - 1s 57us/step - loss: 0.5497 - accuracy: 0.7871 - val_loss: 16626.1501 - val_accuracy: 0.5413\n",
      "Epoch 178/500\n",
      "19062/19062 [==============================] - 1s 55us/step - loss: 0.4658 - accuracy: 0.8011 - val_loss: 20335.7803 - val_accuracy: 0.5000\n",
      "Epoch 179/500\n",
      "19062/19062 [==============================] - 1s 57us/step - loss: 0.4666 - accuracy: 0.8025 - val_loss: 139513.5805 - val_accuracy: 0.5677\n",
      "Epoch 180/500\n",
      "19062/19062 [==============================] - 1s 58us/step - loss: 0.4591 - accuracy: 0.8041 - val_loss: 1678986.1076 - val_accuracy: 0.5287\n",
      "Epoch 181/500\n",
      "19062/19062 [==============================] - 1s 55us/step - loss: 1.9411 - accuracy: 0.8008 - val_loss: 22412.1102 - val_accuracy: 0.6553\n",
      "Epoch 182/500\n",
      "19062/19062 [==============================] - 1s 55us/step - loss: 0.4824 - accuracy: 0.7989 - val_loss: 446481.0164 - val_accuracy: 0.5672\n",
      "Epoch 183/500\n",
      "19062/19062 [==============================] - 1s 57us/step - loss: 4.5442 - accuracy: 0.8018 - val_loss: 2367337.3056 - val_accuracy: 0.5418\n",
      "Epoch 184/500\n",
      "19062/19062 [==============================] - 1s 55us/step - loss: 0.4529 - accuracy: 0.8033 - val_loss: 2252189.7839 - val_accuracy: 0.5582\n",
      "Epoch 185/500\n",
      "19062/19062 [==============================] - 1s 56us/step - loss: 0.4816 - accuracy: 0.7989 - val_loss: 11728.7214 - val_accuracy: 0.5000\n",
      "Epoch 186/500\n",
      "19062/19062 [==============================] - 1s 55us/step - loss: 0.4601 - accuracy: 0.8004 - val_loss: 951827.3081 - val_accuracy: 0.5024\n",
      "Epoch 187/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4782 - accuracy: 0.8026 - val_loss: 512466.0104 - val_accuracy: 0.5882\n",
      "Epoch 188/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.5639 - accuracy: 0.8046 - val_loss: 359941.4730 - val_accuracy: 0.5914\n",
      "Epoch 189/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4670 - accuracy: 0.8032 - val_loss: 51782.8239 - val_accuracy: 0.5000\n",
      "Epoch 190/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4622 - accuracy: 0.8029 - val_loss: 140711.2055 - val_accuracy: 0.5001\n",
      "Epoch 191/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4624 - accuracy: 0.8018 - val_loss: 178997.7512 - val_accuracy: 0.5000\n",
      "Epoch 192/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.6207 - accuracy: 0.8036 - val_loss: 56597.1010 - val_accuracy: 0.5000\n",
      "Epoch 193/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 1.0412 - accuracy: 0.7995 - val_loss: 79333.2224 - val_accuracy: 0.5000\n",
      "Epoch 194/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4619 - accuracy: 0.8025 - val_loss: 52834.4845 - val_accuracy: 0.5000\n",
      "Epoch 195/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4764 - accuracy: 0.7997 - val_loss: 35158.0255 - val_accuracy: 0.5000\n",
      "Epoch 196/500\n",
      "19062/19062 [==============================] - 1s 55us/step - loss: 0.4617 - accuracy: 0.8039 - val_loss: 23889.3805 - val_accuracy: 0.5000\n",
      "Epoch 197/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.5399 - accuracy: 0.8033 - val_loss: 80046.8668 - val_accuracy: 0.5000\n",
      "Epoch 198/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.5062 - accuracy: 0.8022 - val_loss: 33351.1315 - val_accuracy: 0.5000\n",
      "Epoch 199/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 2.0822 - accuracy: 0.8054 - val_loss: 45545.7820 - val_accuracy: 0.5000\n",
      "Epoch 200/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.4623 - accuracy: 0.8036 - val_loss: 32111.8586 - val_accuracy: 0.5000\n",
      "Epoch 201/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.8249 - accuracy: 0.8037 - val_loss: 26273.0869 - val_accuracy: 0.5000\n",
      "Epoch 202/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.6397 - accuracy: 0.7986 - val_loss: 84077.3570 - val_accuracy: 0.5000\n",
      "Epoch 203/500\n",
      "19062/19062 [==============================] - 1s 55us/step - loss: 0.4776 - accuracy: 0.7989 - val_loss: 10186.7141 - val_accuracy: 0.5000\n",
      "Epoch 204/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.5058 - accuracy: 0.8029 - val_loss: 12071.0405 - val_accuracy: 0.5000\n",
      "Epoch 205/500\n",
      "19062/19062 [==============================] - 1s 55us/step - loss: 0.4575 - accuracy: 0.8031 - val_loss: 25023.4695 - val_accuracy: 0.5000\n",
      "Epoch 206/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.4707 - accuracy: 0.8041 - val_loss: 1460354.5704 - val_accuracy: 0.5134\n",
      "Epoch 207/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4577 - accuracy: 0.8052 - val_loss: 135381.9335 - val_accuracy: 0.5000\n",
      "Epoch 208/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4625 - accuracy: 0.8037 - val_loss: 105902.8428 - val_accuracy: 0.5000\n",
      "Epoch 209/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.4621 - accuracy: 0.7985 - val_loss: 3825.3278 - val_accuracy: 0.5000\n",
      "Epoch 210/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.5084 - accuracy: 0.8015 - val_loss: 802838.6058 - val_accuracy: 0.4782\n",
      "Epoch 211/500\n",
      "19062/19062 [==============================] - 1s 55us/step - loss: 0.4559 - accuracy: 0.8045 - val_loss: 34581.6279 - val_accuracy: 0.5000\n",
      "Epoch 212/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4614 - accuracy: 0.8026 - val_loss: 88452.1064 - val_accuracy: 0.5000\n",
      "Epoch 213/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.5554 - accuracy: 0.8013 - val_loss: 32536.9157 - val_accuracy: 0.5000\n",
      "Epoch 214/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.5082 - accuracy: 0.8053 - val_loss: 14462.0574 - val_accuracy: 0.5007\n",
      "Epoch 215/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.5423 - accuracy: 0.8018 - val_loss: 99055.7851 - val_accuracy: 0.4351\n",
      "Epoch 216/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.4667 - accuracy: 0.8035 - val_loss: 33903.9440 - val_accuracy: 0.6082\n",
      "Epoch 217/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.7462 - accuracy: 0.8031 - val_loss: 74187.1367 - val_accuracy: 0.5000\n",
      "Epoch 218/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.4547 - accuracy: 0.8050 - val_loss: 5558.6619 - val_accuracy: 0.5000\n",
      "Epoch 219/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 2.5376 - accuracy: 0.8070 - val_loss: 1.8990 - val_accuracy: 0.5000\n",
      "Epoch 220/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.4932 - accuracy: 0.8028 - val_loss: 8547.6307 - val_accuracy: 0.6342\n",
      "Epoch 221/500\n",
      "19062/19062 [==============================] - 1s 55us/step - loss: 0.4517 - accuracy: 0.8044 - val_loss: 82301.0564 - val_accuracy: 0.5000\n",
      "Epoch 222/500\n",
      "19062/19062 [==============================] - 1s 55us/step - loss: 0.5070 - accuracy: 0.8050 - val_loss: 89452.8820 - val_accuracy: 0.5000\n",
      "Epoch 223/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.4690 - accuracy: 0.8025 - val_loss: 191349.7669 - val_accuracy: 0.6255\n",
      "Epoch 224/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.4713 - accuracy: 0.8031 - val_loss: 2345583.9777 - val_accuracy: 0.5656\n",
      "Epoch 225/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.5671 - accuracy: 0.8028 - val_loss: 15.5045 - val_accuracy: 0.5000\n",
      "Epoch 226/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.5443 - accuracy: 0.8052 - val_loss: 327911.0991 - val_accuracy: 0.5826\n",
      "Epoch 227/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.5001 - accuracy: 0.8052 - val_loss: 213216.5652 - val_accuracy: 0.6208\n",
      "Epoch 228/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.4745 - accuracy: 0.8058 - val_loss: 188493.3196 - val_accuracy: 0.4691\n",
      "Epoch 229/500\n",
      "19062/19062 [==============================] - 1s 56us/step - loss: 0.4580 - accuracy: 0.8045 - val_loss: 7037.4673 - val_accuracy: 0.5000\n",
      "Epoch 230/500\n",
      "19062/19062 [==============================] - 1s 55us/step - loss: 0.4587 - accuracy: 0.8029 - val_loss: 25177.1756 - val_accuracy: 0.5000\n",
      "Epoch 231/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.4690 - accuracy: 0.8084 - val_loss: 178496.4748 - val_accuracy: 0.5000\n",
      "Epoch 232/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.9876 - accuracy: 0.8055 - val_loss: 47932.4685 - val_accuracy: 0.5000\n",
      "Epoch 233/500\n",
      "19062/19062 [==============================] - 1s 57us/step - loss: 0.4598 - accuracy: 0.8024 - val_loss: 65.5296 - val_accuracy: 0.5000\n",
      "Epoch 234/500\n",
      "19062/19062 [==============================] - 1s 57us/step - loss: 0.4691 - accuracy: 0.8038 - val_loss: 61911.1515 - val_accuracy: 0.5000\n",
      "Epoch 235/500\n",
      "19062/19062 [==============================] - 1s 57us/step - loss: 0.4674 - accuracy: 0.8049 - val_loss: 75301.5551 - val_accuracy: 0.5000\n",
      "Epoch 236/500\n",
      "19062/19062 [==============================] - 1s 57us/step - loss: 0.4666 - accuracy: 0.8023 - val_loss: 1.0490 - val_accuracy: 0.5000\n",
      "Epoch 237/500\n",
      "19062/19062 [==============================] - 1s 56us/step - loss: 0.5173 - accuracy: 0.8057 - val_loss: 48924.1256 - val_accuracy: 0.5000\n",
      "Epoch 238/500\n",
      "19062/19062 [==============================] - 1s 58us/step - loss: 0.4520 - accuracy: 0.8040 - val_loss: 132995.3518 - val_accuracy: 0.5000\n",
      "Epoch 239/500\n",
      "19062/19062 [==============================] - 1s 55us/step - loss: 19.6474 - accuracy: 0.8045 - val_loss: 39038.9226 - val_accuracy: 0.5000\n",
      "Epoch 240/500\n",
      "19062/19062 [==============================] - 1s 55us/step - loss: 0.5256 - accuracy: 0.8064 - val_loss: 30383.6938 - val_accuracy: 0.5000\n",
      "Epoch 241/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 1.1309 - accuracy: 0.8052 - val_loss: 39933.4555 - val_accuracy: 0.5000\n",
      "Epoch 242/500\n",
      "19062/19062 [==============================] - 1s 56us/step - loss: 0.4596 - accuracy: 0.8029 - val_loss: 4.2827 - val_accuracy: 0.5000\n",
      "Epoch 243/500\n",
      "19062/19062 [==============================] - 1s 55us/step - loss: 0.4969 - accuracy: 0.8074 - val_loss: 154431.1868 - val_accuracy: 0.5000\n",
      "Epoch 244/500\n",
      "19062/19062 [==============================] - 1s 55us/step - loss: 0.5210 - accuracy: 0.8041 - val_loss: 21596.4189 - val_accuracy: 0.5000\n",
      "Epoch 245/500\n",
      "19062/19062 [==============================] - 1s 57us/step - loss: 0.4771 - accuracy: 0.8056 - val_loss: 133242.3082 - val_accuracy: 0.5000\n",
      "Epoch 246/500\n",
      "19062/19062 [==============================] - 1s 58us/step - loss: 0.4847 - accuracy: 0.8066 - val_loss: 35392.0723 - val_accuracy: 0.5000\n",
      "Epoch 247/500\n",
      "19062/19062 [==============================] - 1s 58us/step - loss: 0.4995 - accuracy: 0.8086 - val_loss: 16979.6276 - val_accuracy: 0.5000\n",
      "Epoch 248/500\n",
      "19062/19062 [==============================] - 1s 56us/step - loss: 1.3487 - accuracy: 0.8041 - val_loss: 1.0868 - val_accuracy: 0.5000\n",
      "Epoch 249/500\n",
      "19062/19062 [==============================] - 1s 56us/step - loss: 0.4890 - accuracy: 0.8033 - val_loss: 1339.8290 - val_accuracy: 0.5000\n",
      "Epoch 250/500\n",
      "19062/19062 [==============================] - 1s 59us/step - loss: 0.4673 - accuracy: 0.8050 - val_loss: 107.3019 - val_accuracy: 0.5000\n",
      "Epoch 251/500\n",
      "19062/19062 [==============================] - 1s 56us/step - loss: 0.4592 - accuracy: 0.8065 - val_loss: 173138.3498 - val_accuracy: 0.5000\n",
      "Epoch 252/500\n",
      "19062/19062 [==============================] - 1s 56us/step - loss: 0.4605 - accuracy: 0.8045 - val_loss: 25279.1185 - val_accuracy: 0.5000\n",
      "Epoch 253/500\n",
      "19062/19062 [==============================] - 1s 57us/step - loss: 0.4619 - accuracy: 0.8034 - val_loss: 17120.9393 - val_accuracy: 0.5000\n",
      "Epoch 254/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4693 - accuracy: 0.8023 - val_loss: 9320.6039 - val_accuracy: 0.5000\n",
      "Epoch 255/500\n",
      "19062/19062 [==============================] - 1s 57us/step - loss: 0.4665 - accuracy: 0.8046 - val_loss: 30461.4542 - val_accuracy: 0.5000\n",
      "Epoch 256/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.6005 - accuracy: 0.8060 - val_loss: 4499.9924 - val_accuracy: 0.5000\n",
      "Epoch 257/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.4580 - accuracy: 0.8034 - val_loss: 18694.6262 - val_accuracy: 0.5000\n",
      "Epoch 258/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.4629 - accuracy: 0.7974 - val_loss: 10034.1336 - val_accuracy: 0.5000\n",
      "Epoch 259/500\n",
      "19062/19062 [==============================] - 1s 56us/step - loss: 0.5428 - accuracy: 0.8044 - val_loss: 6720.5276 - val_accuracy: 0.5000\n",
      "Epoch 260/500\n",
      "19062/19062 [==============================] - 1s 59us/step - loss: 0.4941 - accuracy: 0.8057 - val_loss: 9362.6507 - val_accuracy: 0.5000\n",
      "Epoch 261/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.4577 - accuracy: 0.8084 - val_loss: 3224.5780 - val_accuracy: 0.5000\n",
      "Epoch 262/500\n",
      "19062/19062 [==============================] - 1s 59us/step - loss: 12.6252 - accuracy: 0.8076 - val_loss: 52297.2814 - val_accuracy: 0.5000\n",
      "Epoch 263/500\n",
      "19062/19062 [==============================] - 1s 63us/step - loss: 0.4593 - accuracy: 0.8034 - val_loss: 5225.3283 - val_accuracy: 0.5000\n",
      "Epoch 264/500\n",
      "19062/19062 [==============================] - 1s 61us/step - loss: 0.4831 - accuracy: 0.8036 - val_loss: 1.1023 - val_accuracy: 0.5000\n",
      "Epoch 265/500\n",
      "19062/19062 [==============================] - 1s 60us/step - loss: 0.4612 - accuracy: 0.8029 - val_loss: 13887.8207 - val_accuracy: 0.5000\n",
      "Epoch 266/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19062/19062 [==============================] - 1s 61us/step - loss: 0.4611 - accuracy: 0.8046 - val_loss: 40140.9102 - val_accuracy: 0.5000\n",
      "Epoch 267/500\n",
      "19062/19062 [==============================] - 1s 55us/step - loss: 0.4617 - accuracy: 0.8058 - val_loss: 5091.5350 - val_accuracy: 0.5000\n",
      "Epoch 268/500\n",
      "19062/19062 [==============================] - 1s 52us/step - loss: 0.4544 - accuracy: 0.8087 - val_loss: 38175.1784 - val_accuracy: 0.5000\n",
      "Epoch 269/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4536 - accuracy: 0.8062 - val_loss: 9961.1826 - val_accuracy: 0.5000\n",
      "Epoch 270/500\n",
      "19062/19062 [==============================] - 1s 52us/step - loss: 0.5184 - accuracy: 0.8085 - val_loss: 62186.2840 - val_accuracy: 0.5000\n",
      "Epoch 271/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.4852 - accuracy: 0.8043 - val_loss: 42368.2399 - val_accuracy: 0.5000\n",
      "Epoch 272/500\n",
      "19062/19062 [==============================] - 1s 58us/step - loss: 3.4426 - accuracy: 0.8004 - val_loss: 159457.8841 - val_accuracy: 0.5000\n",
      "Epoch 273/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4524 - accuracy: 0.8076 - val_loss: 44587.0384 - val_accuracy: 0.5000\n",
      "Epoch 274/500\n",
      "19062/19062 [==============================] - 1s 56us/step - loss: 0.4812 - accuracy: 0.8087 - val_loss: 42200.1544 - val_accuracy: 0.5000\n",
      "Epoch 275/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.4587 - accuracy: 0.8066 - val_loss: 9764.7323 - val_accuracy: 0.5000\n",
      "Epoch 276/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4597 - accuracy: 0.8073 - val_loss: 14317.1536 - val_accuracy: 0.5000\n",
      "Epoch 277/500\n",
      "19062/19062 [==============================] - 1s 56us/step - loss: 0.4661 - accuracy: 0.8042 - val_loss: 1.1063 - val_accuracy: 0.5000\n",
      "Epoch 278/500\n",
      "19062/19062 [==============================] - 1s 56us/step - loss: 0.5138 - accuracy: 0.8059 - val_loss: 1.1094 - val_accuracy: 0.5000\n",
      "Epoch 279/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 2.7772 - accuracy: 0.8079 - val_loss: 21636.5618 - val_accuracy: 0.5000\n",
      "Epoch 280/500\n",
      "19062/19062 [==============================] - 1s 56us/step - loss: 0.4504 - accuracy: 0.8074 - val_loss: 6645.8095 - val_accuracy: 0.5000\n",
      "Epoch 281/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4741 - accuracy: 0.8054 - val_loss: 9548.3966 - val_accuracy: 0.5000\n",
      "Epoch 282/500\n",
      "19062/19062 [==============================] - 1s 58us/step - loss: 0.6853 - accuracy: 0.8036 - val_loss: 1.1094 - val_accuracy: 0.5000\n",
      "Epoch 283/500\n",
      "19062/19062 [==============================] - 1s 60us/step - loss: 0.4602 - accuracy: 0.8061 - val_loss: 2348.8069 - val_accuracy: 0.5000\n",
      "Epoch 284/500\n",
      "19062/19062 [==============================] - 1s 55us/step - loss: 0.4593 - accuracy: 0.8079 - val_loss: 1.1221 - val_accuracy: 0.5000\n",
      "Epoch 285/500\n",
      "19062/19062 [==============================] - 1s 56us/step - loss: 0.5623 - accuracy: 0.8050 - val_loss: 38342.1813 - val_accuracy: 0.5000\n",
      "Epoch 286/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.4558 - accuracy: 0.8068 - val_loss: 1.1254 - val_accuracy: 0.5000\n",
      "Epoch 287/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4561 - accuracy: 0.8051 - val_loss: 73469.4835 - val_accuracy: 0.5000\n",
      "Epoch 288/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4723 - accuracy: 0.8055 - val_loss: 3000.8315 - val_accuracy: 0.5000\n",
      "Epoch 289/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4984 - accuracy: 0.8046 - val_loss: 101830.7138 - val_accuracy: 0.5000\n",
      "Epoch 290/500\n",
      "19062/19062 [==============================] - 1s 52us/step - loss: 0.6496 - accuracy: 0.8087 - val_loss: 10747.9425 - val_accuracy: 0.5000\n",
      "Epoch 291/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4682 - accuracy: 0.8064 - val_loss: 39409.6178 - val_accuracy: 0.5000\n",
      "Epoch 292/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4699 - accuracy: 0.8053 - val_loss: 7233.4447 - val_accuracy: 0.5000\n",
      "Epoch 293/500\n",
      "19062/19062 [==============================] - 1s 55us/step - loss: 0.4502 - accuracy: 0.8069 - val_loss: 33891.6948 - val_accuracy: 0.5000\n",
      "Epoch 294/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 1.4441 - accuracy: 0.8073 - val_loss: 25315.0493 - val_accuracy: 0.5000\n",
      "Epoch 295/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4528 - accuracy: 0.8068 - val_loss: 30856.9992 - val_accuracy: 0.5000\n",
      "Epoch 296/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.6933 - accuracy: 0.8082 - val_loss: 24986.6671 - val_accuracy: 0.5000\n",
      "Epoch 297/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.4529 - accuracy: 0.8057 - val_loss: 25678.9021 - val_accuracy: 0.5000\n",
      "Epoch 298/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.4976 - accuracy: 0.8048 - val_loss: 17864.1542 - val_accuracy: 0.5000\n",
      "Epoch 299/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4554 - accuracy: 0.8073 - val_loss: 108469.0677 - val_accuracy: 0.5000\n",
      "Epoch 300/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 1.5279 - accuracy: 0.8073 - val_loss: 118505.8387 - val_accuracy: 0.5000\n",
      "Epoch 301/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4483 - accuracy: 0.8082 - val_loss: 47653.7858 - val_accuracy: 0.5000\n",
      "Epoch 302/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4802 - accuracy: 0.8099 - val_loss: 9532.1697 - val_accuracy: 0.5000\n",
      "Epoch 303/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.8027 - accuracy: 0.8071 - val_loss: 9731.8943 - val_accuracy: 0.5000\n",
      "Epoch 304/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.4763 - accuracy: 0.8067 - val_loss: 1.1352 - val_accuracy: 0.5000\n",
      "Epoch 305/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.9632 - accuracy: 0.8067 - val_loss: 3169.7239 - val_accuracy: 0.5000\n",
      "Epoch 306/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.4608 - accuracy: 0.8074 - val_loss: 84567.6980 - val_accuracy: 0.5000\n",
      "Epoch 307/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.5043 - accuracy: 0.8060 - val_loss: 40433.6108 - val_accuracy: 0.5000\n",
      "Epoch 308/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4583 - accuracy: 0.8085 - val_loss: 11724.9067 - val_accuracy: 0.5000\n",
      "Epoch 309/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.4568 - accuracy: 0.8079 - val_loss: 25900.5540 - val_accuracy: 0.5000\n",
      "Epoch 310/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4571 - accuracy: 0.8071 - val_loss: 11783.0377 - val_accuracy: 0.5000\n",
      "Epoch 311/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4570 - accuracy: 0.8054 - val_loss: 1.1397 - val_accuracy: 0.5000\n",
      "Epoch 312/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4495 - accuracy: 0.8086 - val_loss: 19035.3591 - val_accuracy: 0.5000\n",
      "Epoch 313/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4912 - accuracy: 0.8088 - val_loss: 10252.7702 - val_accuracy: 0.5000\n",
      "Epoch 314/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.4995 - accuracy: 0.8099 - val_loss: 3230.8823 - val_accuracy: 0.5000\n",
      "Epoch 315/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.4534 - accuracy: 0.8059 - val_loss: 11797.1779 - val_accuracy: 0.5000\n",
      "Epoch 316/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4714 - accuracy: 0.8055 - val_loss: 5172.0342 - val_accuracy: 0.5000\n",
      "Epoch 317/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4546 - accuracy: 0.8050 - val_loss: 1.1474 - val_accuracy: 0.5000\n",
      "Epoch 318/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.5523 - accuracy: 0.8076 - val_loss: 53546.8500 - val_accuracy: 0.5000\n",
      "Epoch 319/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4507 - accuracy: 0.8075 - val_loss: 15490.5674 - val_accuracy: 0.5000\n",
      "Epoch 320/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4581 - accuracy: 0.8074 - val_loss: 8355.5938 - val_accuracy: 0.5000\n",
      "Epoch 321/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.5241 - accuracy: 0.8090 - val_loss: 11371.8407 - val_accuracy: 0.5000\n",
      "Epoch 322/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.4958 - accuracy: 0.8068 - val_loss: 2848.4677 - val_accuracy: 0.5000\n",
      "Epoch 323/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4902 - accuracy: 0.8090 - val_loss: 1.1396 - val_accuracy: 0.5000\n",
      "Epoch 324/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.4712 - accuracy: 0.8044 - val_loss: 1.1424 - val_accuracy: 0.5000\n",
      "Epoch 325/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4507 - accuracy: 0.8067 - val_loss: 2317.8012 - val_accuracy: 0.5000\n",
      "Epoch 326/500\n",
      "19062/19062 [==============================] - 1s 52us/step - loss: 3.0980 - accuracy: 0.8078 - val_loss: 5540.6812 - val_accuracy: 0.5000\n",
      "Epoch 327/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.4570 - accuracy: 0.8085 - val_loss: 69926.1133 - val_accuracy: 0.5000\n",
      "Epoch 328/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4641 - accuracy: 0.8047 - val_loss: 74584.8158 - val_accuracy: 0.5000\n",
      "Epoch 329/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4606 - accuracy: 0.8075 - val_loss: 23232.6649 - val_accuracy: 0.5000\n",
      "Epoch 330/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.4812 - accuracy: 0.8056 - val_loss: 1.1411 - val_accuracy: 0.5000\n",
      "Epoch 331/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4538 - accuracy: 0.8054 - val_loss: 1.1432 - val_accuracy: 0.5000\n",
      "Epoch 332/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4829 - accuracy: 0.8077 - val_loss: 1.1424 - val_accuracy: 0.5000\n",
      "Epoch 333/500\n",
      "19062/19062 [==============================] - 1s 56us/step - loss: 0.6648 - accuracy: 0.8073 - val_loss: 1.1386 - val_accuracy: 0.5000\n",
      "Epoch 334/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 1.4563 - accuracy: 0.8082 - val_loss: 1.1403 - val_accuracy: 0.5000\n",
      "Epoch 335/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 1.1128 - accuracy: 0.8090 - val_loss: 8.5436 - val_accuracy: 0.5000\n",
      "Epoch 336/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.4501 - accuracy: 0.8080 - val_loss: 9649.2934 - val_accuracy: 0.5000\n",
      "Epoch 337/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4781 - accuracy: 0.8079 - val_loss: 1.1451 - val_accuracy: 0.5000\n",
      "Epoch 338/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4572 - accuracy: 0.8094 - val_loss: 1.1464 - val_accuracy: 0.5000\n",
      "Epoch 339/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.5049 - accuracy: 0.8116 - val_loss: 621.8716 - val_accuracy: 0.5000\n",
      "Epoch 340/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.8173 - accuracy: 0.8029 - val_loss: 1.1490 - val_accuracy: 0.5000\n",
      "Epoch 341/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4548 - accuracy: 0.8101 - val_loss: 1.1509 - val_accuracy: 0.5000\n",
      "Epoch 342/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.4571 - accuracy: 0.8074 - val_loss: 34053.8368 - val_accuracy: 0.5000\n",
      "Epoch 343/500\n",
      "19062/19062 [==============================] - 1s 55us/step - loss: 0.5962 - accuracy: 0.8113 - val_loss: 4297.2961 - val_accuracy: 0.5000\n",
      "Epoch 344/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.5005 - accuracy: 0.8101 - val_loss: 5300.0917 - val_accuracy: 0.5000\n",
      "Epoch 345/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.5196 - accuracy: 0.8109 - val_loss: 2795.2183 - val_accuracy: 0.5000\n",
      "Epoch 346/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4636 - accuracy: 0.8085 - val_loss: 6045.3983 - val_accuracy: 0.5000\n",
      "Epoch 347/500\n",
      "19062/19062 [==============================] - 1s 55us/step - loss: 0.4817 - accuracy: 0.8095 - val_loss: 1020916.4694 - val_accuracy: 0.5000\n",
      "Epoch 348/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.5310 - accuracy: 0.8071 - val_loss: 1063.8556 - val_accuracy: 0.5000\n",
      "Epoch 349/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4589 - accuracy: 0.8089 - val_loss: 471.7447 - val_accuracy: 0.5000\n",
      "Epoch 350/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4561 - accuracy: 0.8096 - val_loss: 207437.5485 - val_accuracy: 0.5000\n",
      "Epoch 351/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.4599 - accuracy: 0.8104 - val_loss: 1.1597 - val_accuracy: 0.5000\n",
      "Epoch 352/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.6788 - accuracy: 0.8138 - val_loss: 109760.7050 - val_accuracy: 0.5000\n",
      "Epoch 353/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.5715 - accuracy: 0.8108 - val_loss: 10.6883 - val_accuracy: 0.5000\n",
      "Epoch 354/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.9646 - accuracy: 0.8096 - val_loss: 1.8584 - val_accuracy: 0.5001\n",
      "Epoch 355/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4796 - accuracy: 0.8064 - val_loss: 1.1696 - val_accuracy: 0.5000\n",
      "Epoch 356/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.5115 - accuracy: 0.8098 - val_loss: 1.1705 - val_accuracy: 0.5000\n",
      "Epoch 357/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 1.7868 - accuracy: 0.8092 - val_loss: 2.9397 - val_accuracy: 0.5000\n",
      "Epoch 358/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4583 - accuracy: 0.8081 - val_loss: 1.1626 - val_accuracy: 0.5000\n",
      "Epoch 359/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4621 - accuracy: 0.8078 - val_loss: 1.1592 - val_accuracy: 0.5000\n",
      "Epoch 360/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.4655 - accuracy: 0.8088 - val_loss: 1.1623 - val_accuracy: 0.5000\n",
      "Epoch 361/500\n",
      "19062/19062 [==============================] - 1s 55us/step - loss: 0.5017 - accuracy: 0.8056 - val_loss: 1.1594 - val_accuracy: 0.5000\n",
      "Epoch 362/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.4769 - accuracy: 0.8068 - val_loss: 373362.8714 - val_accuracy: 0.5000\n",
      "Epoch 363/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.5071 - accuracy: 0.7983 - val_loss: 3789.9477 - val_accuracy: 0.5000\n",
      "Epoch 364/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4617 - accuracy: 0.8044 - val_loss: 9520.4431 - val_accuracy: 0.5000\n",
      "Epoch 365/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4667 - accuracy: 0.8088 - val_loss: 17397.8743 - val_accuracy: 0.5000\n",
      "Epoch 366/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 2.6135 - accuracy: 0.8101 - val_loss: 47420.6088 - val_accuracy: 0.5000\n",
      "Epoch 367/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.6243 - accuracy: 0.8087 - val_loss: 68564.9731 - val_accuracy: 0.5000\n",
      "Epoch 368/500\n",
      "19062/19062 [==============================] - 1s 57us/step - loss: 0.6489 - accuracy: 0.8122 - val_loss: 49352.8171 - val_accuracy: 0.5000\n",
      "Epoch 369/500\n",
      "19062/19062 [==============================] - 1s 55us/step - loss: 0.4659 - accuracy: 0.8085 - val_loss: 6837.3494 - val_accuracy: 0.5000\n",
      "Epoch 370/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.4740 - accuracy: 0.8090 - val_loss: 1.1584 - val_accuracy: 0.5000\n",
      "Epoch 371/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 1.4253 - accuracy: 0.8056 - val_loss: 1.1582 - val_accuracy: 0.5000\n",
      "Epoch 372/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.4582 - accuracy: 0.8101 - val_loss: 1.1562 - val_accuracy: 0.5000\n",
      "Epoch 373/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19062/19062 [==============================] - 1s 56us/step - loss: 0.4766 - accuracy: 0.8101 - val_loss: 10.4156 - val_accuracy: 0.5000\n",
      "Epoch 374/500\n",
      "19062/19062 [==============================] - 1s 58us/step - loss: 0.4891 - accuracy: 0.8106 - val_loss: 1.1607 - val_accuracy: 0.5000\n",
      "Epoch 375/500\n",
      "19062/19062 [==============================] - 1s 55us/step - loss: 0.4575 - accuracy: 0.8100 - val_loss: 205438.4209 - val_accuracy: 0.5000\n",
      "Epoch 376/500\n",
      "19062/19062 [==============================] - 1s 55us/step - loss: 0.4623 - accuracy: 0.8057 - val_loss: 473284.2791 - val_accuracy: 0.5000\n",
      "Epoch 377/500\n",
      "19062/19062 [==============================] - 1s 58us/step - loss: 0.4605 - accuracy: 0.8103 - val_loss: 1.1582 - val_accuracy: 0.5000\n",
      "Epoch 378/500\n",
      "19062/19062 [==============================] - 1s 56us/step - loss: 0.4484 - accuracy: 0.8117 - val_loss: 86539.5731 - val_accuracy: 0.5000\n",
      "Epoch 379/500\n",
      "19062/19062 [==============================] - 1s 56us/step - loss: 0.4638 - accuracy: 0.8106 - val_loss: 25236.5151 - val_accuracy: 0.5000\n",
      "Epoch 380/500\n",
      "19062/19062 [==============================] - 1s 59us/step - loss: 0.5426 - accuracy: 0.8120 - val_loss: 1.1570 - val_accuracy: 0.5000\n",
      "Epoch 381/500\n",
      "19062/19062 [==============================] - 1s 58us/step - loss: 0.6055 - accuracy: 0.8086 - val_loss: 10.3791 - val_accuracy: 0.5000\n",
      "Epoch 382/500\n",
      "19062/19062 [==============================] - 1s 57us/step - loss: 0.4707 - accuracy: 0.8118 - val_loss: 126465.7450 - val_accuracy: 0.4517\n",
      "Epoch 383/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.5391 - accuracy: 0.8110 - val_loss: 44649.4149 - val_accuracy: 0.5000\n",
      "Epoch 384/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.4571 - accuracy: 0.8097 - val_loss: 48498.0279 - val_accuracy: 0.5000\n",
      "Epoch 385/500\n",
      "19062/19062 [==============================] - 1s 56us/step - loss: 0.4600 - accuracy: 0.8058 - val_loss: 4686.7412 - val_accuracy: 0.5000\n",
      "Epoch 386/500\n",
      "19062/19062 [==============================] - 1s 59us/step - loss: 1.1762 - accuracy: 0.8044 - val_loss: 120746639.2734 - val_accuracy: 0.5000\n",
      "Epoch 387/500\n",
      "19062/19062 [==============================] - 1s 59us/step - loss: 0.7590 - accuracy: 0.8089 - val_loss: 1069128.1200 - val_accuracy: 0.5000\n",
      "Epoch 388/500\n",
      "19062/19062 [==============================] - 1s 61us/step - loss: 0.4760 - accuracy: 0.8116 - val_loss: 12009374.4566 - val_accuracy: 0.5000\n",
      "Epoch 389/500\n",
      "19062/19062 [==============================] - 1s 57us/step - loss: 0.4607 - accuracy: 0.8090 - val_loss: 2.6248 - val_accuracy: 0.5000\n",
      "Epoch 390/500\n",
      "19062/19062 [==============================] - 1s 56us/step - loss: 0.8061 - accuracy: 0.8098 - val_loss: 37281.3061 - val_accuracy: 0.5000\n",
      "Epoch 391/500\n",
      "19062/19062 [==============================] - 1s 56us/step - loss: 0.4585 - accuracy: 0.8106 - val_loss: 21648.3815 - val_accuracy: 0.5000\n",
      "Epoch 392/500\n",
      "19062/19062 [==============================] - 1s 56us/step - loss: 0.4595 - accuracy: 0.8050 - val_loss: 106020.6524 - val_accuracy: 0.5000\n",
      "Epoch 393/500\n",
      "19062/19062 [==============================] - 1s 56us/step - loss: 0.4516 - accuracy: 0.8081 - val_loss: 18430.4048 - val_accuracy: 0.5000\n",
      "Epoch 394/500\n",
      "19062/19062 [==============================] - 1s 57us/step - loss: 0.9587 - accuracy: 0.8094 - val_loss: 38060.9667 - val_accuracy: 0.5000\n",
      "Epoch 395/500\n",
      "19062/19062 [==============================] - 1s 56us/step - loss: 0.4625 - accuracy: 0.8077 - val_loss: 1.1421 - val_accuracy: 0.5000\n",
      "Epoch 396/500\n",
      "19062/19062 [==============================] - 1s 56us/step - loss: 0.5318 - accuracy: 0.8083 - val_loss: 1989.6294 - val_accuracy: 0.5000\n",
      "Epoch 397/500\n",
      "19062/19062 [==============================] - 1s 57us/step - loss: 0.4504 - accuracy: 0.8086 - val_loss: 1.1434 - val_accuracy: 0.5000\n",
      "Epoch 398/500\n",
      "19062/19062 [==============================] - 1s 57us/step - loss: 0.4592 - accuracy: 0.8096 - val_loss: 1.1480 - val_accuracy: 0.5000\n",
      "Epoch 399/500\n",
      "19062/19062 [==============================] - 1s 56us/step - loss: 0.4560 - accuracy: 0.8084 - val_loss: 3772.2906 - val_accuracy: 0.5000\n",
      "Epoch 400/500\n",
      "19062/19062 [==============================] - 1s 56us/step - loss: 0.4584 - accuracy: 0.8097 - val_loss: 1624.5205 - val_accuracy: 0.5000\n",
      "Epoch 401/500\n",
      "19062/19062 [==============================] - 1s 58us/step - loss: 0.4721 - accuracy: 0.8100 - val_loss: 1.1440 - val_accuracy: 0.5000\n",
      "Epoch 402/500\n",
      "19062/19062 [==============================] - 1s 57us/step - loss: 0.4563 - accuracy: 0.8072 - val_loss: 1.1840 - val_accuracy: 0.5000\n",
      "Epoch 403/500\n",
      "19062/19062 [==============================] - 1s 58us/step - loss: 0.4482 - accuracy: 0.8119 - val_loss: 5001.9723 - val_accuracy: 0.5000\n",
      "Epoch 404/500\n",
      "19062/19062 [==============================] - 1s 57us/step - loss: 0.4520 - accuracy: 0.8093 - val_loss: 2472.5649 - val_accuracy: 0.5000\n",
      "Epoch 405/500\n",
      "19062/19062 [==============================] - 1s 58us/step - loss: 0.4598 - accuracy: 0.8093 - val_loss: 441367.7630 - val_accuracy: 0.5000\n",
      "Epoch 406/500\n",
      "19062/19062 [==============================] - 1s 56us/step - loss: 8.8231 - accuracy: 0.8084 - val_loss: 1.1472 - val_accuracy: 0.5000\n",
      "Epoch 407/500\n",
      "19062/19062 [==============================] - 1s 56us/step - loss: 0.5339 - accuracy: 0.8096 - val_loss: 195066.7302 - val_accuracy: 0.5000\n",
      "Epoch 408/500\n",
      "19062/19062 [==============================] - 1s 57us/step - loss: 0.5920 - accuracy: 0.8127 - val_loss: 468.7480 - val_accuracy: 0.6017\n",
      "Epoch 409/500\n",
      "19062/19062 [==============================] - 1s 57us/step - loss: 3.7636 - accuracy: 0.8118 - val_loss: 1.1447 - val_accuracy: 0.5000\n",
      "Epoch 410/500\n",
      "19062/19062 [==============================] - 1s 56us/step - loss: 0.4530 - accuracy: 0.8110 - val_loss: 0.8735 - val_accuracy: 0.6257\n",
      "Epoch 411/500\n",
      "19062/19062 [==============================] - 1s 56us/step - loss: 0.4571 - accuracy: 0.8104 - val_loss: 1.1472 - val_accuracy: 0.5000\n",
      "Epoch 412/500\n",
      "19062/19062 [==============================] - 1s 55us/step - loss: 0.5067 - accuracy: 0.8084 - val_loss: 3.8326 - val_accuracy: 0.5000\n",
      "Epoch 413/500\n",
      "19062/19062 [==============================] - 1s 57us/step - loss: 0.5273 - accuracy: 0.8082 - val_loss: 1.1495 - val_accuracy: 0.5000\n",
      "Epoch 414/500\n",
      "19062/19062 [==============================] - 1s 60us/step - loss: 2.8924 - accuracy: 0.8066 - val_loss: 1.1485 - val_accuracy: 0.5000\n",
      "Epoch 415/500\n",
      "19062/19062 [==============================] - 1s 56us/step - loss: 0.4559 - accuracy: 0.8082 - val_loss: 1.1534 - val_accuracy: 0.5000\n",
      "Epoch 416/500\n",
      "19062/19062 [==============================] - 1s 58us/step - loss: 5.7584 - accuracy: 0.8135 - val_loss: 1.7934 - val_accuracy: 0.5000\n",
      "Epoch 417/500\n",
      "19062/19062 [==============================] - 1s 58us/step - loss: 0.4535 - accuracy: 0.8106 - val_loss: 44.3574 - val_accuracy: 0.5000\n",
      "Epoch 418/500\n",
      "19062/19062 [==============================] - 1s 59us/step - loss: 0.4591 - accuracy: 0.8110 - val_loss: 2.8320 - val_accuracy: 0.5385\n",
      "Epoch 419/500\n",
      "19062/19062 [==============================] - 1s 57us/step - loss: 0.5327 - accuracy: 0.8046 - val_loss: 0.9110 - val_accuracy: 0.5000\n",
      "Epoch 420/500\n",
      "19062/19062 [==============================] - 1s 57us/step - loss: 0.6086 - accuracy: 0.8062 - val_loss: 6.6862 - val_accuracy: 0.5947\n",
      "Epoch 421/500\n",
      "19062/19062 [==============================] - 1s 56us/step - loss: 0.4656 - accuracy: 0.8086 - val_loss: 7.3665 - val_accuracy: 0.5988\n",
      "Epoch 422/500\n",
      "19062/19062 [==============================] - 1s 58us/step - loss: 0.4731 - accuracy: 0.8045 - val_loss: 78.6200 - val_accuracy: 0.6275\n",
      "Epoch 423/500\n",
      "19062/19062 [==============================] - 1s 58us/step - loss: 0.8698 - accuracy: 0.8110 - val_loss: 138.3318 - val_accuracy: 0.5000\n",
      "Epoch 424/500\n",
      "19062/19062 [==============================] - 1s 58us/step - loss: 0.4745 - accuracy: 0.8115 - val_loss: 17.9077 - val_accuracy: 0.5000\n",
      "Epoch 425/500\n",
      "19062/19062 [==============================] - 1s 60us/step - loss: 0.4611 - accuracy: 0.8090 - val_loss: 26781.1732 - val_accuracy: 0.5000\n",
      "Epoch 426/500\n",
      "19062/19062 [==============================] - 1s 60us/step - loss: 4.0443 - accuracy: 0.8094 - val_loss: 1.1628 - val_accuracy: 0.6213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 427/500\n",
      "19062/19062 [==============================] - 1s 57us/step - loss: 0.4615 - accuracy: 0.8084 - val_loss: 0.8523 - val_accuracy: 0.6488\n",
      "Epoch 428/500\n",
      "19062/19062 [==============================] - 1s 57us/step - loss: 0.4597 - accuracy: 0.8118 - val_loss: 5.1039 - val_accuracy: 0.5000\n",
      "Epoch 429/500\n",
      "19062/19062 [==============================] - 1s 56us/step - loss: 0.8822 - accuracy: 0.8119 - val_loss: 55343.0523 - val_accuracy: 0.5000\n",
      "Epoch 430/500\n",
      "19062/19062 [==============================] - 1s 55us/step - loss: 0.6891 - accuracy: 0.8107 - val_loss: 621291.7275 - val_accuracy: 0.5000\n",
      "Epoch 431/500\n",
      "19062/19062 [==============================] - 1s 55us/step - loss: 0.5446 - accuracy: 0.8128 - val_loss: 2190.6708 - val_accuracy: 0.5000\n",
      "Epoch 432/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.4898 - accuracy: 0.8125 - val_loss: 46824.6727 - val_accuracy: 0.5000\n",
      "Epoch 433/500\n",
      "19062/19062 [==============================] - 1s 57us/step - loss: 15.4542 - accuracy: 0.8132 - val_loss: 145315.2795 - val_accuracy: 0.5000\n",
      "Epoch 434/500\n",
      "19062/19062 [==============================] - 1s 55us/step - loss: 0.4484 - accuracy: 0.8121 - val_loss: 451781.8513 - val_accuracy: 0.5000\n",
      "Epoch 435/500\n",
      "19062/19062 [==============================] - 1s 55us/step - loss: 0.4805 - accuracy: 0.8142 - val_loss: 10964.2777 - val_accuracy: 0.5000\n",
      "Epoch 436/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4560 - accuracy: 0.8077 - val_loss: 35737.9737 - val_accuracy: 0.5000\n",
      "Epoch 437/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4674 - accuracy: 0.8103 - val_loss: 69527.3026 - val_accuracy: 0.5000\n",
      "Epoch 438/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.4493 - accuracy: 0.8097 - val_loss: 2219130.5245 - val_accuracy: 0.6833\n",
      "Epoch 439/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4820 - accuracy: 0.8130 - val_loss: 50235523.7235 - val_accuracy: 0.6057\n",
      "Epoch 440/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 7.9391 - accuracy: 0.8106 - val_loss: 2.5616 - val_accuracy: 0.5827\n",
      "Epoch 441/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.4748 - accuracy: 0.8140 - val_loss: 112.3632 - val_accuracy: 0.6068\n",
      "Epoch 442/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4506 - accuracy: 0.8084 - val_loss: 24.8552 - val_accuracy: 0.6122\n",
      "Epoch 443/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.4993 - accuracy: 0.8080 - val_loss: 4.7803 - val_accuracy: 0.5186\n",
      "Epoch 444/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 2.5636 - accuracy: 0.8115 - val_loss: 19.1480 - val_accuracy: 0.5847\n",
      "Epoch 445/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4744 - accuracy: 0.8108 - val_loss: 1.5765 - val_accuracy: 0.5371\n",
      "Epoch 446/500\n",
      "19062/19062 [==============================] - 1s 55us/step - loss: 2.6670 - accuracy: 0.8105 - val_loss: 7.5993 - val_accuracy: 0.5803\n",
      "Epoch 447/500\n",
      "19062/19062 [==============================] - 1s 56us/step - loss: 0.4899 - accuracy: 0.8091 - val_loss: 1.1516 - val_accuracy: 0.5017\n",
      "Epoch 448/500\n",
      "19062/19062 [==============================] - 1s 56us/step - loss: 0.6133 - accuracy: 0.8075 - val_loss: 8395905.3457 - val_accuracy: 0.5209\n",
      "Epoch 449/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 2.2873 - accuracy: 0.8081 - val_loss: 2.7106 - val_accuracy: 0.5179\n",
      "Epoch 450/500\n",
      "19062/19062 [==============================] - 1s 59us/step - loss: 0.4498 - accuracy: 0.8138 - val_loss: 1.0523 - val_accuracy: 0.5007\n",
      "Epoch 451/500\n",
      "19062/19062 [==============================] - 1s 62us/step - loss: 0.4600 - accuracy: 0.8077 - val_loss: 6162.6533 - val_accuracy: 0.5337\n",
      "Epoch 452/500\n",
      "19062/19062 [==============================] - 1s 55us/step - loss: 0.4769 - accuracy: 0.8100 - val_loss: 1.0785 - val_accuracy: 0.5066\n",
      "Epoch 453/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.6220 - accuracy: 0.8089 - val_loss: 6.4220 - val_accuracy: 0.5467\n",
      "Epoch 454/500\n",
      "19062/19062 [==============================] - 1s 55us/step - loss: 0.7589 - accuracy: 0.8116 - val_loss: 0.9744 - val_accuracy: 0.5006\n",
      "Epoch 455/500\n",
      "19062/19062 [==============================] - 1s 57us/step - loss: 0.9452 - accuracy: 0.8058 - val_loss: 6.6591 - val_accuracy: 0.5765\n",
      "Epoch 456/500\n",
      "19062/19062 [==============================] - 1s 62us/step - loss: 0.4813 - accuracy: 0.7961 - val_loss: 0.8941 - val_accuracy: 0.5612\n",
      "Epoch 457/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.5303 - accuracy: 0.7999 - val_loss: 1.2378 - val_accuracy: 0.5739\n",
      "Epoch 458/500\n",
      "19062/19062 [==============================] - 1s 55us/step - loss: 0.5243 - accuracy: 0.8094 - val_loss: 1.0661 - val_accuracy: 0.5004\n",
      "Epoch 459/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.5545 - accuracy: 0.8095 - val_loss: 0.9740 - val_accuracy: 0.5351\n",
      "Epoch 460/500\n",
      "19062/19062 [==============================] - 1s 56us/step - loss: 0.4536 - accuracy: 0.8119 - val_loss: 0.9639 - val_accuracy: 0.5298\n",
      "Epoch 461/500\n",
      "19062/19062 [==============================] - 1s 56us/step - loss: 0.4681 - accuracy: 0.8045 - val_loss: 0.8489 - val_accuracy: 0.5491\n",
      "Epoch 462/500\n",
      "19062/19062 [==============================] - 1s 68us/step - loss: 0.5508 - accuracy: 0.8088 - val_loss: 1.6496 - val_accuracy: 0.5774\n",
      "Epoch 463/500\n",
      "19062/19062 [==============================] - 1s 70us/step - loss: 0.5795 - accuracy: 0.7981 - val_loss: 3.8794 - val_accuracy: 0.6036\n",
      "Epoch 464/500\n",
      "19062/19062 [==============================] - 1s 71us/step - loss: 0.5369 - accuracy: 0.8033 - val_loss: 2.3773 - val_accuracy: 0.5006\n",
      "Epoch 465/500\n",
      "19062/19062 [==============================] - 1s 60us/step - loss: 0.5207 - accuracy: 0.7961 - val_loss: 1.0820 - val_accuracy: 0.5603\n",
      "Epoch 466/500\n",
      "19062/19062 [==============================] - 1s 61us/step - loss: 0.4802 - accuracy: 0.7995 - val_loss: 11495.7269 - val_accuracy: 0.5000\n",
      "Epoch 467/500\n",
      "19062/19062 [==============================] - 1s 64us/step - loss: 0.4773 - accuracy: 0.7985 - val_loss: 13180.9697 - val_accuracy: 0.5000\n",
      "Epoch 468/500\n",
      "19062/19062 [==============================] - 1s 56us/step - loss: 0.5037 - accuracy: 0.8053 - val_loss: 7746.8954 - val_accuracy: 0.5000\n",
      "Epoch 469/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4924 - accuracy: 0.7959 - val_loss: 1386.1065 - val_accuracy: 0.5000\n",
      "Epoch 470/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.5494 - accuracy: 0.8088 - val_loss: 3278.5591 - val_accuracy: 0.5000\n",
      "Epoch 471/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.4604 - accuracy: 0.8086 - val_loss: 6451.9573 - val_accuracy: 0.5000\n",
      "Epoch 472/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.4568 - accuracy: 0.8066 - val_loss: 2623.5304 - val_accuracy: 0.5000\n",
      "Epoch 473/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4771 - accuracy: 0.8104 - val_loss: 12219.5709 - val_accuracy: 0.5000\n",
      "Epoch 474/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4980 - accuracy: 0.8128 - val_loss: 1.0914 - val_accuracy: 0.5000\n",
      "Epoch 475/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.6056 - accuracy: 0.8061 - val_loss: 8577.6220 - val_accuracy: 0.5000\n",
      "Epoch 476/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4598 - accuracy: 0.8100 - val_loss: 34.4405 - val_accuracy: 0.6167\n",
      "Epoch 477/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.5198 - accuracy: 0.8131 - val_loss: 11.7004 - val_accuracy: 0.6149\n",
      "Epoch 478/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4684 - accuracy: 0.8082 - val_loss: 87974.4180 - val_accuracy: 0.5000\n",
      "Epoch 479/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.5934 - accuracy: 0.8033 - val_loss: 103.5346 - val_accuracy: 0.6780\n",
      "Epoch 480/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 0.6911 - accuracy: 0.8101 - val_loss: 58441.6725 - val_accuracy: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 481/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4972 - accuracy: 0.8112 - val_loss: 66238.0429 - val_accuracy: 0.5000\n",
      "Epoch 482/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4901 - accuracy: 0.8091 - val_loss: 1.0837 - val_accuracy: 0.5000\n",
      "Epoch 483/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4580 - accuracy: 0.8096 - val_loss: 18545.5917 - val_accuracy: 0.5000\n",
      "Epoch 484/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 2.9993 - accuracy: 0.8147 - val_loss: 10824.4452 - val_accuracy: 0.5000\n",
      "Epoch 485/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.5128 - accuracy: 0.8110 - val_loss: 3729.1897 - val_accuracy: 0.5000\n",
      "Epoch 486/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4987 - accuracy: 0.8103 - val_loss: 1.1512 - val_accuracy: 0.5017\n",
      "Epoch 487/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 2.0802 - accuracy: 0.8122 - val_loss: 316.4047 - val_accuracy: 0.6662\n",
      "Epoch 488/500\n",
      "19062/19062 [==============================] - 1s 55us/step - loss: 0.4559 - accuracy: 0.8095 - val_loss: 8610.9485 - val_accuracy: 0.5000\n",
      "Epoch 489/500\n",
      "19062/19062 [==============================] - 1s 55us/step - loss: 0.4878 - accuracy: 0.8115 - val_loss: 11471.8660 - val_accuracy: 0.5000\n",
      "Epoch 490/500\n",
      "19062/19062 [==============================] - 1s 55us/step - loss: 2.7754 - accuracy: 0.8056 - val_loss: 1292526.2753 - val_accuracy: 0.5000\n",
      "Epoch 491/500\n",
      "19062/19062 [==============================] - 1s 55us/step - loss: 0.5226 - accuracy: 0.8097 - val_loss: 24561.5127 - val_accuracy: 0.5000\n",
      "Epoch 492/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.4592 - accuracy: 0.8105 - val_loss: 28433.3164 - val_accuracy: 0.5000\n",
      "Epoch 493/500\n",
      "19062/19062 [==============================] - 1s 54us/step - loss: 13.4357 - accuracy: 0.8143 - val_loss: 6905.7906 - val_accuracy: 0.5000\n",
      "Epoch 494/500\n",
      "19062/19062 [==============================] - 1s 58us/step - loss: 0.5208 - accuracy: 0.8127 - val_loss: 65691416.4545 - val_accuracy: 0.5000\n",
      "Epoch 495/500\n",
      "19062/19062 [==============================] - 1s 53us/step - loss: 0.5380 - accuracy: 0.8115 - val_loss: 17368.1888 - val_accuracy: 0.5000\n",
      "Epoch 496/500\n",
      "19062/19062 [==============================] - 1s 55us/step - loss: 2.5784 - accuracy: 0.8084 - val_loss: 14676609.1597 - val_accuracy: 0.5000\n",
      "Epoch 497/500\n",
      "19062/19062 [==============================] - 1s 59us/step - loss: 0.7321 - accuracy: 0.8046 - val_loss: 28594.7389 - val_accuracy: 0.5000\n",
      "Epoch 498/500\n",
      "19062/19062 [==============================] - 1s 56us/step - loss: 3.7163 - accuracy: 0.8079 - val_loss: 470058.7972 - val_accuracy: 0.5000\n",
      "Epoch 499/500\n",
      "19062/19062 [==============================] - 1s 55us/step - loss: 0.4583 - accuracy: 0.8058 - val_loss: 404803.4752 - val_accuracy: 0.5000\n",
      "Epoch 500/500\n",
      "19062/19062 [==============================] - 1s 57us/step - loss: 0.4874 - accuracy: 0.8074 - val_loss: 130531.2935 - val_accuracy: 0.5000\n"
     ]
    }
   ],
   "source": [
    "opt = optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "network.compile(optimizer='rmsprop',\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "epochs = 500\n",
    "history = network.fit(nn_train_x, \n",
    "                      train_y, \n",
    "                      epochs=epochs, \n",
    "                      batch_size=128, \n",
    "                      validation_data=(nn_test_x, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "loss_values = history_dict['loss']\n",
    "test_loss_values = history_dict['val_loss']\n",
    "epochs_range = range(1, epochs + 1)\n",
    "plt.plot(epochs_range, loss_values, 'bo', label='Training loss')\n",
    "plt.plot(epochs_range, test_loss_values, 'ro', label='Test loss')\n",
    "plt.title('Training and test loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc_values = history_dict['accuracy']\n",
    "test_acc_values = history_dict['val_accuracy']\n",
    "plt.plot(epochs_range, acc_values, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs_range, test_acc_values, 'ro', label='Test accuracy')\n",
    "plt.title('Training and test accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 3573 0.5\n",
      "Pos: 0 0.0\n",
      "Star Correct: 3573 0.5\n",
      "True Positive: 0 0.0\n",
      "True Negative: 3573 0.5\n",
      "False Positive: 0 0.0\n",
      "False Negative: 3573 0.5\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "pos_count = 0\n",
    "star_cor = 0\n",
    "matrix = [0, 0, 0, 0]\n",
    "preds = network.predict(nn_test_x)\n",
    "for i in range(len(preds)):\n",
    "    if (preds[i] > 0.5):\n",
    "        pos_count += 1\n",
    "    if (preds[i] == test_y.iloc[i]):\n",
    "        star_cor += 1\n",
    "    if (preds[i] > 0.5 and test_y.iloc[i] > 0.5):\n",
    "        correct += 1\n",
    "        matrix[0] += 1\n",
    "    elif (preds[i] <= 0.5 and test_y.iloc[i] <= 0.5):\n",
    "        correct += 1\n",
    "        matrix[1] += 1\n",
    "    elif (preds[i] > 0.5 and test_y.iloc[i] <= 0.5):\n",
    "        matrix[2] += 1\n",
    "    elif (preds[i] <= 0.5 and test_y.iloc[i] > 0.5):\n",
    "        matrix[3] += 1\n",
    "\n",
    "print('Correct:', correct, correct / len(test_df))\n",
    "print('Pos:', pos_count, pos_count / len(test_df))\n",
    "print('Star Correct:', star_cor, star_cor / len(test_df))\n",
    "print('True Positive:', matrix[0], matrix[0] / len(test_df))\n",
    "print('True Negative:', matrix[1], matrix[1] / len(test_df))\n",
    "print('False Positive:', matrix[2], matrix[2] / len(test_df))\n",
    "print('False Negative:', matrix[3], matrix[3] / len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#network.save('basic_neural_net3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
